{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "BAYcDP2PHM-C",
      "metadata": {
        "id": "BAYcDP2PHM-C"
      },
      "source": [
        "# **Second Model**\n",
        "(Based on current understanding of task)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "from torch import nn\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "HiwN_wEO9oZ1"
      },
      "id": "HiwN_wEO9oZ1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "8PhxdHtB94nB"
      },
      "id": "8PhxdHtB94nB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=models.vgg16(weights=models.VGG16_Weights.DEFAULT)"
      ],
      "metadata": {
        "id": "olNODvlQr5jn"
      },
      "id": "olNODvlQr5jn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5e5yq-tscIT",
        "outputId": "efdbf26e-507b-402f-f81b-79bd3a3f6244"
      },
      "id": "m5e5yq-tscIT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU(inplace=True)\n",
            "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (8): ReLU(inplace=True)\n",
            "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (11): ReLU(inplace=True)\n",
            "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (13): ReLU(inplace=True)\n",
            "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (15): ReLU(inplace=True)\n",
            "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (18): ReLU(inplace=True)\n",
            "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (20): ReLU(inplace=True)\n",
            "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (22): ReLU(inplace=True)\n",
            "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (25): ReLU(inplace=True)\n",
            "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (27): ReLU(inplace=True)\n",
            "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (29): ReLU(inplace=True)\n",
            "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3RCIMe8sdoh",
        "outputId": "69a13a00-96b8-49b8-adda-2858272cfc94"
      },
      "id": "y3RCIMe8sdoh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Dropout(p=0.5, inplace=False)\n",
            "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "  (4): ReLU(inplace=True)\n",
            "  (5): Dropout(p=0.5, inplace=False)\n",
            "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.features[30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSY_lFPhs2rK",
        "outputId": "e304944b-1ce0-45f2-b323-128e252a928f"
      },
      "id": "qSY_lFPhs2rK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx8X0Grcv2sh",
        "outputId": "421f3170-ca31-4916-df9a-9e191edbdb3d"
      },
      "id": "Qx8X0Grcv2sh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Identity()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = torch.ones((2,3,224,224))\n",
        "print(a(b).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfHgS0BHv5Yx",
        "outputId": "086933f0-879c-4335-af56-e0243f0f948b"
      },
      "id": "XfHgS0BHv5Yx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 25088])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "e7pt362n98AA"
      },
      "id": "e7pt362n98AA"
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "zhtuH40S9-l6"
      },
      "id": "zhtuH40S9-l6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AAAAAAAAAAAA"
      ],
      "metadata": {
        "id": "_sq8DvrMtiCA"
      },
      "id": "_sq8DvrMtiCA"
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "uWBOiSCethXl"
      },
      "id": "uWBOiSCethXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "HZkhPctO10SQ",
      "metadata": {
        "id": "HZkhPctO10SQ"
      },
      "source": [
        "## Upper Branch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### simplified version"
      ],
      "metadata": {
        "id": "RUiAvU-06beH"
      },
      "id": "RUiAvU-06beH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sOJU_2qUHgSo",
      "metadata": {
        "id": "sOJU_2qUHgSo"
      },
      "outputs": [],
      "source": [
        "class GroundBranchSim(nn.Module):\n",
        "  def __init__(self, use_seg=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.use_seg = use_seg\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):#, self.vgg1.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    # Modify size of input\n",
        "    self.vgg1.features[0] = nn.Conv2d(\n",
        "        1,\n",
        "        64,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    # Initiate weights\n",
        "    nn.init.kaiming_normal_(self.vgg1.features[0].weight)\n",
        "\n",
        "    if self.use_seg:\n",
        "      # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "      self.FNN = nn.Sequential(\n",
        "        nn.Linear(2000, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "      )\n",
        "    else:\n",
        "      # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "      self.FNN = nn.Sequential(\n",
        "        nn.Linear(1000, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "      )\n",
        "\n",
        "  def forward(self, ground_view, segmented_ground):\n",
        "    x_ground = self.vgg2(ground_view)\n",
        "\n",
        "    if self.use_seg:\n",
        "      x_segmented = self.vgg3(segmented_ground)\n",
        "      x = torch.cat((x_ground, x_segmented), dim=-1)\n",
        "    else:\n",
        "      x = x_ground\n",
        "\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### amplified version"
      ],
      "metadata": {
        "id": "XZz-LuaU85cV"
      },
      "id": "XZz-LuaU85cV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYGaGGSI9IvM"
      },
      "outputs": [],
      "source": [
        "class GroundBranch(nn.Module):\n",
        "  def __init__(self, use_seg=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.use_seg = use_seg\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):#, self.vgg1.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # Modify size of input\n",
        "    self.vgg1.features[0] = nn.Conv2d(\n",
        "        1,\n",
        "        64,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    # Initiate weights\n",
        "    nn.init.kaiming_normal_(self.vgg1.features[0].weight)\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    # Remove fully connected layer\n",
        "    self.vgg1.classifier = nn.Sequential(Identity())\n",
        "    self.vgg2.classifier = nn.Sequential(Identity())\n",
        "\n",
        "    self.conv = nn.Conv2d(512,512,1)\n",
        "    self.sigma = nn.Sigmoid()\n",
        "    self.GAP = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    if self.use_seg:\n",
        "      # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "      self.FNN = nn.Sequential(\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "      )\n",
        "    else:\n",
        "      # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "      self.FNN = nn.Sequential(\n",
        "        nn.Linear(1024, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "      )\n",
        "\n",
        "  def forward(self, ground_view, segmented_ground):\n",
        "    x_ground = self.vgg2(ground_view).view(-1,512,7,7)\n",
        "\n",
        "    if self.use_seg:\n",
        "      x_segmented = self.vgg3(segmented_ground).view(-1,512,7,7)\n",
        "\n",
        "      mask = self.conv(x_segmented)\n",
        "      x_ground *= mask\n",
        "\n",
        "      x_ground = self.GAP(x_ground)\n",
        "      x_ground = x_ground.view(x_ground.shape[0], -1) # return to Bx512\n",
        "      x_aerial = self.GAP(x_aerial)\n",
        "      x_aerial = x_aerial.view(x_aerial.shape[0], -1)\n",
        "\n",
        "      x = torch.cat((x_ground, x_segmented), dim=-1) # Bx1024\n",
        "    else:\n",
        "      x = self.GAP(x_ground)\n",
        "      x = x.view(x.shape[0], -1)\n",
        "\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ],
      "id": "IYGaGGSI9IvM"
    },
    {
      "cell_type": "markdown",
      "id": "hDh1-Kx6HaDt",
      "metadata": {
        "id": "hDh1-Kx6HaDt"
      },
      "source": [
        "## Lower Branch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### simplified version"
      ],
      "metadata": {
        "id": "sLDBkPz76VnM"
      },
      "id": "sLDBkPz76VnM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZGMfYo512Bz9",
      "metadata": {
        "id": "ZGMfYo512Bz9"
      },
      "outputs": [],
      "source": [
        "class AerialBranchSim(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # Modify size of input\n",
        "    self.vgg1.features[0] = nn.Conv2d(\n",
        "        1,\n",
        "        64,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    # Initiate weights\n",
        "    nn.init.kaiming_normal_(self.vgg1.features[0].weight)\n",
        "\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "    self.FNN = nn.Sequential(\n",
        "        nn.Linear(3000, 2048),\n",
        "        nn.LayerNorm(2048),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(2048, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "    )\n",
        "\n",
        "  def forward(self, ground_view, synthetic_aerial, segmented_aerial, candidate_aerial):\n",
        "    #x_ground = self.vgg1(ground_view)\n",
        "    x_segmented = self.vgg1(segmented_aerial)\n",
        "    x_synthetic = self.vgg2(synthetic_aerial)\n",
        "    x_candidate = self.vgg2(candidate_aerial)\n",
        "\n",
        "    x = torch.cat((x_synthetic, x_segmented, x_candidate), dim=-1)\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### amplified version"
      ],
      "metadata": {
        "id": "iAq7RVvR8-I3"
      },
      "id": "iAq7RVvR8-I3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdlUnPCF9E76"
      },
      "outputs": [],
      "source": [
        "class AerialBranch(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # Modify size of input\n",
        "    self.vgg1.features[0] = nn.Conv2d(\n",
        "        1,\n",
        "        64,\n",
        "        kernel_size=3,\n",
        "        padding=1\n",
        "    )\n",
        "    # Initiate weights\n",
        "    nn.init.kaiming_normal_(self.vgg1.features[0].weight)\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    # Remove fully connected layer\n",
        "    self.vgg1.classifier = nn.Sequential(Identity())\n",
        "    self.vgg2.classifier = nn.Sequential(Identity())\n",
        "\n",
        "    self.conv = nn.Conv2d(512,512,1)\n",
        "    self.sigma = nn.Sigmoid()\n",
        "    self.GAP = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "    # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "    self.FNN = nn.Sequential(\n",
        "        nn.Linear(1536, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "    )\n",
        "\n",
        "  def forward(self, ground_view, synthetic_aerial, segmented_aerial, candidate_aerial):\n",
        "    #x_ground = self.vgg1(ground_view)\n",
        "    x_segmented = self.vgg1(segmented_aerial).view(-1,512,7,7)\n",
        "    x_synthetic = self.vgg2(synthetic_aerial).view(-1,512,7,7)\n",
        "    x_candidate = self.vgg2(candidate_aerial).view(-1,512,7,7)\n",
        "\n",
        "    mask = self.conv(x_segmented)\n",
        "\n",
        "    x_candidate *= mask\n",
        "    x_synthetic *= mask\n",
        "\n",
        "    x_candidate = self.GAP(x_candidate)\n",
        "    x_candidate = x_candidate.view(x_candidate.shape[0], -1)\n",
        "    x_synthetic = self.GAP(x_synthetic)\n",
        "    x_synthetic = x_synthetic.view(x_synthetic.shape[0], -1)\n",
        "    x_segmented = self.GAP(x_segmented)\n",
        "    x_segmented = x_segmented.view(x_segmented.shape[0], -1)\n",
        "\n",
        "    x = torch.cat((x_synthetic, x_segmented, x_candidate), dim=-1)\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ],
      "id": "rdlUnPCF9E76"
    },
    {
      "cell_type": "markdown",
      "id": "1b6MaaxAMktk",
      "metadata": {
        "id": "1b6MaaxAMktk"
      },
      "source": [
        "## Complete Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L-rqm6NRM1RJ",
      "metadata": {
        "id": "L-rqm6NRM1RJ"
      },
      "outputs": [],
      "source": [
        "class CompNet(nn.Module):\n",
        "  def __init__(self, ground_branch=GroundBranchSim(), aerial_branch=AerialBranchSim()):\n",
        "    super().__init__()\n",
        "    self.GB = ground_branch\n",
        "    self.AB = aerial_branch\n",
        "\n",
        "  def forward(self, ground_view, segmented_ground, synthetic_aerial, segmented_aerial, candidate_aerial):\n",
        "    return self.GB(ground_view, segmented_ground), self.AB(synthetic_aerial, segmented_aerial, candidate_aerial)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rTCQCntdCfw7",
      "metadata": {
        "id": "rTCQCntdCfw7"
      },
      "source": [
        "## Triplet Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HwWxmTnMCkrZ",
      "metadata": {
        "id": "HwWxmTnMCkrZ"
      },
      "outputs": [],
      "source": [
        "class WeightedSoftMarginTripletLoss(nn.Module):\n",
        "  def __init__(self, margin=0.2):\n",
        "    super().__init__()\n",
        "    self.margin = margin\n",
        "\n",
        "  def forward(self, anchor, positive, negatives):\n",
        "\n",
        "    first = torch.norm(anchor - positive, dim=-1, keepdim=True)\n",
        "    second = -torch.norm(anchor - negatives, dim=-1, keepdim=True)\n",
        "    arg = self.margin * (second+first)\n",
        "    const = torch.zeros((arg.shape[0],1))\n",
        "    arg = torch.cat((const,arg), dim=-1)\n",
        "\n",
        "    return torch.logsumexp(arg, dim=-1).mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KiKP8hBywFAV",
      "metadata": {
        "id": "KiKP8hBywFAV"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hmwUIftSwEWI",
      "metadata": {
        "id": "hmwUIftSwEWI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion=WeightedSoftMarginTripletLoss(), scheduler=None):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, segmented_ground, synthetic_aerial, segmented_aerial in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      segmented_ground = segmented_ground.to(device)\n",
        "      synthetic_aerial = synthetic_aerial.to(device)\n",
        "      segmented_aerial =  segmented_aerial.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      optimizer.zero_grad()       # resets gradients from previous batch\n",
        "      labels, predictions = model( ground, segmented_ground, synthetic_aerial, segmented_aerial, aerial)\n",
        "      #print(aerial_pred.shape)\n",
        "\n",
        "      batch_size = len(labels)\n",
        "      batch_loss = 0.0\n",
        "\n",
        "      for i in range(batch_size):\n",
        "        indeces = range(batch_size)\n",
        "        indeces.pop(i)\n",
        "        anchor = labels[i]\n",
        "        positive = predictions[i]\n",
        "        negatives = predictions(indeces)\n",
        "\n",
        "        # Sanity check\n",
        "        if negatives.shape[0] != batch_size-1:\n",
        "          print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
        "\n",
        "        batch_loss += criterion(anchor, positive, negatives)\n",
        "        #Backward pass\n",
        "      batch_loss.backward()         # computes gradients via backpropagation\n",
        "      optimizer.step()        # updates weights using gradients\n",
        "      #scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "\n",
        "      tot_loss += batch_loss.item()/batch_size\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device, criterion=WeightedSoftMarginTripletLoss()):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, segmented_ground, synthetic_aerial, segmented_aerial in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      segmented_ground = segmented_ground.to(device)\n",
        "      synthetic_aerial = synthetic_aerial.to(device)\n",
        "      segmented_aerial =  segmented_aerial.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      labels, predictions = model( ground, segmented_ground, synthetic_aerial, segmented_aerial, aerial)\n",
        "      #print(aerial_pred.shape)\n",
        "\n",
        "      batch_size = len(labels)\n",
        "      batch_loss = 0.0\n",
        "\n",
        "      for i in range(batch_size):\n",
        "        indeces = range(batch_size)\n",
        "        indeces.pop(i)\n",
        "        anchor = labels[i]\n",
        "        positive = predictions[i]\n",
        "        negatives = predictions(indeces)\n",
        "\n",
        "        # Sanity check\n",
        "        if negatives.shape[0] != batch_size-1:\n",
        "          print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
        "\n",
        "        batch_loss += criterion(anchor, positive, negatives)\n",
        "\n",
        "      tot_loss += batch_loss.item()/batch_size\n",
        "\n",
        "    return tot_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CompNet()"
      ],
      "metadata": {
        "id": "Did9uvdgJ8-s"
      },
      "id": "Did9uvdgJ8-s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hwFdbzaf2xDH",
      "metadata": {
        "id": "hwFdbzaf2xDH"
      },
      "outputs": [],
      "source": [
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}