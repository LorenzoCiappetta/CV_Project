{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e82572b3",
      "metadata": {
        "id": "e82572b3"
      },
      "source": [
        "# **Transformer model**\n",
        "From query groumd view images generate saellite images (natural and segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e9a4ed",
      "metadata": {
        "id": "14e9a4ed"
      },
      "source": [
        "**TODO**:\n",
        "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
        "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "170e556e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "170e556e",
        "outputId": "02f4565b-ba8c-42c7-ce18-405565b74ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision transformers scikit-learn pytorch_lightning -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49e9082c",
      "metadata": {
        "id": "49e9082c"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9bcdc708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bcdc708",
        "outputId": "db4e83c3-7990-4d1c-fac2-308dc2402ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "#import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pylight\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "GTOcTL-h1Mkl"
      },
      "id": "GTOcTL-h1Mkl",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "862b2eb7",
      "metadata": {
        "id": "862b2eb7"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d0a046",
      "metadata": {
        "id": "99d0a046"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4e2ed9de",
      "metadata": {
        "id": "4e2ed9de"
      },
      "outputs": [],
      "source": [
        "# TODO: Need to add segmentation of generated images into dataset...\n",
        "\n",
        "class CVUSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ground_dir, aerial_dir, triplet_list, img_size=224, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ground_dir: Directory with all the ground view images\n",
        "            aerial_dir: Directory with all the ground aerial images\n",
        "            split: 'train', 'val' or 'test'\n",
        "            img_size: Size for images, 224x224\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "        \"\"\"\n",
        "\n",
        "        self.ground_dir = ground_dir\n",
        "        self.aerial_dir = aerial_dir\n",
        "        self.triplet_list = triplet_list\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Default transforms if none provided\n",
        "        if transform is None:\n",
        "            # For ground view images\n",
        "            self.ground_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial images (we might want different processing)\n",
        "            self.aerial_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # Segmentation transform (nearest neighbor resize)\n",
        "            self.segmentation_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size),\n",
        "                interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                transforms.PILToTensor(),\n",
        "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.ground_transform = transform\n",
        "            self.aerial_transform = transform\n",
        "            self.segmentation_transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        aerial_rel, ground_rel, seg_rel = self.triplet_list[idx]\n",
        "\n",
        "        # Load images\n",
        "        for i in range(3):\n",
        "          try:\n",
        "            ground_img = Image.open(self.ground_dir + ground_rel)\n",
        "            aerial_img = Image.open(self.aerial_dir + aerial_rel)\n",
        "            seg_map = Image.open(self.ground_dir + seg_rel)\n",
        "          except:\n",
        "            if i < 2:\n",
        "              continue\n",
        "            else:\n",
        "              raise Exception(\"Drive isn't Drive-ing\")\n",
        "\n",
        "\n",
        "        # Apply transforms\n",
        "        ground_tensor = self.ground_transform(ground_img)\n",
        "        aerial_tensor = self.aerial_transform(aerial_img)\n",
        "        seg_tensor = self.segmentation_transform(seg_map)  # Shape [H, W]\n",
        "\n",
        "        return ground_tensor, aerial_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "75801a76",
      "metadata": {
        "id": "75801a76"
      },
      "outputs": [],
      "source": [
        "def read_triplets_csv(csv_path):\n",
        "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
        "    triplets = []\n",
        "    with open(csv_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            triplets.append((\n",
        "                parts[0].strip(),  # aerial path\n",
        "                parts[1].strip(),  # ground path\n",
        "                parts[2].strip()   # seg path (ground view segmented map)\n",
        "            ))\n",
        "    return triplets\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    ground_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/bingmap/\"\n",
        "else:\n",
        "    ground_dir = \"./CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"./CV_dataset/CVPR_subset/bingmap/\"\n",
        "\n",
        "\n",
        "train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/train-19zl.csv\")\n",
        "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/val-19zl.csv\")        # test set\n",
        "\n",
        "train_dataset = CVUSADataset(ground_dir, aerial_dir, train_triplets)\n",
        "val_dataset = CVUSADataset(ground_dir, aerial_dir, val_triplets)\n",
        "test_dataset = CVUSADataset(ground_dir, aerial_dir, test_triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ee5893c0",
      "metadata": {
        "id": "ee5893c0"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cca9a2f",
      "metadata": {
        "id": "2cca9a2f"
      },
      "source": [
        "## model  \n",
        "\n",
        "```\n",
        "                                                       ---> Aerial Decoder  \n",
        "                                                     /  \n",
        "Ground Image --> Patch Embedding --> ViT Encoder ---  \n",
        "                                                     \\  \n",
        "                                                       ---> Segmentation Decoder  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fdd99998",
      "metadata": {
        "id": "fdd99998"
      },
      "outputs": [],
      "source": [
        "class GroundToAerialTransformer(nn.Module):\n",
        "    def __init__(self, num_seg_classes=7, pretrained=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_seg_classes: Number of segmentation classes\n",
        "            pretrained: Use pretrained ViT weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # ViT Encoder (shared backbone)\n",
        "        model_name = 'google/vit-base-patch16-224-in21k'        # ViT base model, 16x16 patches, 224x224 input size\n",
        "        self.vit_config = ViTConfig.from_pretrained(model_name)\n",
        "        if pretrained:\n",
        "            self.vit = ViTModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.vit = ViTModel(self.vit_config)\n",
        "\n",
        "        # Aerial Image Decoder\n",
        "        self.aerial_decoder = nn.Sequential(\n",
        "            # First upsample to 14x14 (from 197x768)\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 512, kernel_size=2, stride=2),      # convolution\n",
        "            nn.BatchNorm2d(512),        # batch normalization\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 28x28\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 56x56\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample to 224x224\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),     # 3 output channels (RGB)\n",
        "            nn.Tanh()  # Output in [-1, 1] range\n",
        "        )\n",
        "\n",
        "        # Segmentation Head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            # First upsample\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 256, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second upsample\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Third upsample\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(32, num_seg_classes, kernel_size=3, padding=1),       # num_seg_classes output channels (number of segmentation classes)\n",
        "            nn.Softmax(dim=1)  # Multi-class probabilities\n",
        "        )\n",
        "\n",
        "        # Learnable positional embedding for aerial reconstruction\n",
        "        self.aerial_pos_embed = nn.Parameter(torch.zeros(1, 196, self.vit_config.hidden_size))      # 196 = 14x14 (number of patches)\n",
        "        nn.init.trunc_normal_(self.aerial_pos_embed, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode ground image with ViT (process image into patch of tokens)\n",
        "        vit_outputs = self.vit(x)       # Output shape: [batch, 197, hidden_size]\n",
        "\n",
        "        last_hidden_state = vit_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        # remove CLS token for image generation (ViT outputs [CLS] token + 196 patch tokens)\n",
        "        aerial_tokens = last_hidden_state[:, 1:]\n",
        "\n",
        "        # add learned positional embedding for aerial structure\n",
        "        aerial_tokens = aerial_tokens + self.aerial_pos_embed\n",
        "\n",
        "        # Reshape to spatial dimensions (14x14)\n",
        "        batch_size = aerial_tokens.size(0)\n",
        "        aerial_tokens = aerial_tokens.view(batch_size, 14, 14, -1)      # convert 1D sequence into 2D spatial grid. shape becomes: (batch_size, 14, 14, hidden_size)\n",
        "        aerial_tokens = aerial_tokens.permute(0, 3, 1, 2)  # permute shape: (batch_size, hidden_size, 14, 14)\n",
        "\n",
        "        #print(aerial_tokens.shape)\n",
        "        # Decode aerial image\n",
        "        aerial_output = self.aerial_decoder(aerial_tokens)\n",
        "        #print(aerial_output.shape)\n",
        "\n",
        "        # Decode segmentation map\n",
        "        #seg_output = self.segmentation_head(aerial_tokens)\n",
        "\n",
        "\n",
        "        return aerial_output#, seg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lightning Wrapper"
      ],
      "metadata": {
        "id": "3uPQGks-1u8k"
      },
      "id": "3uPQGks-1u8k"
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningWrapper(pylight.LightningModule):\n",
        "  def __init__(self, device, model=GroundToAerialTransformer(num_seg_classes=5)):\n",
        "    super().__init__()\n",
        "    self.dvc=device\n",
        "\n",
        "    self.model=model\n",
        "    self.criterion=nn.L1Loss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    ground, aerial, _ = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "    # Forward pass\n",
        "    aerial_pred = model(ground)\n",
        "    # Compute loss\n",
        "    loss = self.criterion(aerial_pred, aerial)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    ground, aerial, _ = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "    # Forward pass\n",
        "    aerial_pred = model(ground)\n",
        "    # Compute loss\n",
        "    loss = self.criterion(aerial_pred, aerial)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return {\"val_loss\":loss}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': self.model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "        {'params': self.model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "        {'params': self.model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "        {'params': self.model.aerial_pos_embed, 'lr': 1e-4}\n",
        "      ], weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "    return {\"optimizer\":optimizer, \"lr_cheduler\":scheduler}\n"
      ],
      "metadata": {
        "id": "y8eHYjnj1usT"
      },
      "id": "y8eHYjnj1usT",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1208a685",
      "metadata": {
        "id": "1208a685"
      },
      "source": [
        "\n",
        "## training w/ segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40e3731",
      "metadata": {
        "id": "f40e3731"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).cuda()\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true, seg_pred, seg_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    seg_loss = seg_loss_fn(seg_pred, seg_true)\n",
        "    # Weighted combination\n",
        "    return 0.7 * img_loss + 0.3 * seg_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380e48ef",
      "metadata": {
        "id": "380e48ef"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ground, (aerial, seg) in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial, seg_pred, seg)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e1e3a8",
      "metadata": {
        "id": "86e1e3a8"
      },
      "source": [
        "## Training no segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No lightning"
      ],
      "metadata": {
        "id": "vi_-hH3n6jjW"
      },
      "id": "vi_-hH3n6jjW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4d9bdb",
      "metadata": {
        "id": "0b4d9bdb"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).to(device)\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    # Weighted combination\n",
        "    return img_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141dca48",
      "metadata": {
        "id": "141dca48"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, _ in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred = model(ground)\n",
        "        #print(aerial_pred.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial)\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for ground, aerial, _ in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      aerial_pred = model(ground)\n",
        "\n",
        "      loss = total_loss(aerial_pred, aerial)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"content/drive/MyDrive/SavedModels/CV_transformer_1ep\")"
      ],
      "metadata": {
        "id": "4QymIl83pB7U"
      },
      "id": "4QymIl83pB7U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yes lightning"
      ],
      "metadata": {
        "id": "QjejFRq16nHM"
      },
      "id": "QjejFRq16nHM"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightningWrapper(device, model=GroundToAerialTransformer(num_seg_classes=5)).to(device)\n",
        "\n",
        "log_path = \"content/drive/MyDrive/SavedModels/CV_transformer/first_try\"\n",
        "ckpt_path = log_path+\"checkpoints\"\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=ckpt_path,\n",
        "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:-2f}\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_last=True,\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    enable_checkpointing=True,\n",
        "    default_root_dir=log_path,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    enable_progress_bar=True\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "d3c4894816bf4f159be7c715eb4ba541",
            "1583b60ff228444e8b53a8a97f7eb8e9",
            "b13ea40fdc364529a46475f0dc741b81",
            "1158537e5dfb45e7a5470ea0d15ae0da",
            "b5b0d0a9785b477cbe6e7cad16650edb",
            "f845c92747d14b1fa022ae7bc1bc8542",
            "126eb14d07a24e9087c2f8b433e0fc80",
            "0af5b4e1dbb4440c8d51acc2dbd06b8f",
            "a67da40281ec4e5287ca4258eddae404",
            "7104053437d84d63b7996e03f8e6d588",
            "25d8979b76eb49fc8a8685fa7df249df",
            "f3bdbf9be4524ae1a376b166b57abe43",
            "e4e08092c79c40b282119f448dff10e1",
            "6feddd9a5e0449c3bc0719f0a14f018c",
            "4be37f3769f449ce88084febf2bc9a71",
            "20a8574195b74e3fa4151e2e9868b058",
            "a71937fa432f4a1e9cddf9e6fe44c933",
            "999ca02623254c3188a1f8b2c04cedab",
            "5bb5829dd94d4d53a9af0cf61d500505",
            "f720083fb43d4734904c1c0afb965142",
            "bd90e8f65275487bb048c5efd8b5bfbe",
            "c276fc44891e43389bf4f26394c8075d"
          ]
        },
        "id": "GwQgIMRZ6rem",
        "outputId": "900fe61a-485b-4b98-cdd6-8650d2dc47dc"
      },
      "id": "GwQgIMRZ6rem",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type                      | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | model     | GroundToAerialTransformer | 92.6 M | train\n",
            "1 | criterion | L1Loss                    | 0      | train\n",
            "----------------------------------------------------------------\n",
            "92.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "92.6 M    Total params\n",
            "370.568   Total estimated model params size (MB)\n",
            "32        Modules in train mode\n",
            "215       Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d3c4894816bf4f159be7c715eb4ba541"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3bdbf9be4524ae1a376b166b57abe43"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498e350d",
      "metadata": {
        "id": "498e350d"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf8d9f0",
      "metadata": {
        "id": "1bf8d9f0"
      },
      "outputs": [],
      "source": [
        "model.model.eval()\n",
        "with torch.no_grad():\n",
        "  for ground, (aerial, _) in test_loader:\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    aerial_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "    ground = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Satellite Image RGB')\n",
        "    plt.imshow(aerial)     # Original full-size image\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title('Satellite prediction')\n",
        "    plt.imshow(aerial_pred)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title('Ground')\n",
        "    plt.imshow(ground)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d3c4894816bf4f159be7c715eb4ba541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1583b60ff228444e8b53a8a97f7eb8e9",
              "IPY_MODEL_b13ea40fdc364529a46475f0dc741b81",
              "IPY_MODEL_1158537e5dfb45e7a5470ea0d15ae0da"
            ],
            "layout": "IPY_MODEL_b5b0d0a9785b477cbe6e7cad16650edb"
          }
        },
        "1583b60ff228444e8b53a8a97f7eb8e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f845c92747d14b1fa022ae7bc1bc8542",
            "placeholder": "​",
            "style": "IPY_MODEL_126eb14d07a24e9087c2f8b433e0fc80",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "b13ea40fdc364529a46475f0dc741b81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0af5b4e1dbb4440c8d51acc2dbd06b8f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a67da40281ec4e5287ca4258eddae404",
            "value": 2
          }
        },
        "1158537e5dfb45e7a5470ea0d15ae0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7104053437d84d63b7996e03f8e6d588",
            "placeholder": "​",
            "style": "IPY_MODEL_25d8979b76eb49fc8a8685fa7df249df",
            "value": " 2/2 [00:25&lt;00:00,  0.08it/s]"
          }
        },
        "b5b0d0a9785b477cbe6e7cad16650edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "f845c92747d14b1fa022ae7bc1bc8542": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "126eb14d07a24e9087c2f8b433e0fc80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0af5b4e1dbb4440c8d51acc2dbd06b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a67da40281ec4e5287ca4258eddae404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7104053437d84d63b7996e03f8e6d588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25d8979b76eb49fc8a8685fa7df249df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3bdbf9be4524ae1a376b166b57abe43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4e08092c79c40b282119f448dff10e1",
              "IPY_MODEL_6feddd9a5e0449c3bc0719f0a14f018c",
              "IPY_MODEL_4be37f3769f449ce88084febf2bc9a71"
            ],
            "layout": "IPY_MODEL_20a8574195b74e3fa4151e2e9868b058"
          }
        },
        "e4e08092c79c40b282119f448dff10e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a71937fa432f4a1e9cddf9e6fe44c933",
            "placeholder": "​",
            "style": "IPY_MODEL_999ca02623254c3188a1f8b2c04cedab",
            "value": "Epoch 0:   2%"
          }
        },
        "6feddd9a5e0449c3bc0719f0a14f018c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bb5829dd94d4d53a9af0cf61d500505",
            "max": 3776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f720083fb43d4734904c1c0afb965142",
            "value": 60
          }
        },
        "4be37f3769f449ce88084febf2bc9a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd90e8f65275487bb048c5efd8b5bfbe",
            "placeholder": "​",
            "style": "IPY_MODEL_c276fc44891e43389bf4f26394c8075d",
            "value": " 60/3776 [24:33&lt;25:20:58,  0.04it/s, v_num=4, train_loss=0.613]"
          }
        },
        "20a8574195b74e3fa4151e2e9868b058": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a71937fa432f4a1e9cddf9e6fe44c933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "999ca02623254c3188a1f8b2c04cedab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bb5829dd94d4d53a9af0cf61d500505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f720083fb43d4734904c1c0afb965142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd90e8f65275487bb048c5efd8b5bfbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c276fc44891e43389bf4f26394c8075d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}