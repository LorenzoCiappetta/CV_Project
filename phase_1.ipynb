{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82572b3",
   "metadata": {},
   "source": [
    "# **Transformer model**\n",
    "From query groumd view images generate saellite images (natural and segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9a4ed",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
    "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170e556e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision transformers scikit-learn pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e9082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Colab: False\n"
     ]
    }
   ],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "print(\"Running in Colab:\", is_colab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcdc708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "import pytorch_lightning as pylight\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from torch import amp\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0a046",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ed9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVUSADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, triplet_list, img_size=224, transform=None):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.triplet_list = triplet_list\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Default transforms if none provided\n",
    "        if transform is None:\n",
    "            # For ground view images\n",
    "            self.ground_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
    "            ])\n",
    "            # For aerial images\n",
    "            self.aerial_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
    "            ])\n",
    "            # For aerial segmentation maps\n",
    "            self.segmentation_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
    "            ])\n",
    "        else:\n",
    "            self.ground_transform = transform\n",
    "            self.aerial_transform = transform\n",
    "            self.segmentation_transform = transform\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplet_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ground_rel, aerial_rel, seg_rel = self.triplet_list[idx]\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Load images\n",
    "                ground_img = Image.open(self.dataset_dir + ground_rel).convert('RGB')\n",
    "                aerial_img = Image.open(self.dataset_dir + aerial_rel).convert('RGB')\n",
    "                seg_map = Image.open(self.dataset_dir + seg_rel)\n",
    "\n",
    "                # Break if successful\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading images for index {idx}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "\n",
    "\n",
    "        # Apply transforms\n",
    "        ground_tensor = self.ground_transform(ground_img)\n",
    "        aerial_tensor = self.aerial_transform(aerial_img)\n",
    "        seg_tensor = self.segmentation_transform(seg_map)\n",
    "\n",
    "        return ground_tensor, aerial_tensor, seg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75801a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_triplets_csv(csv_path):\n",
    "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
    "    triplets = []\n",
    "    with open(csv_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            triplets.append((\n",
    "                parts[0].strip(),  # aerial path\n",
    "                parts[1].strip(),  # ground path\n",
    "                parts[2].strip()   # seg path (ground view segmented map)\n",
    "            ))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    dataset_dir = \"/content/drive/MyDrive/CVUSA_subset/\"\n",
    "else:\n",
    "    dataset_dir = \"./CVUSA_subset\"\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/train.csv\")\n",
    "    train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
    "    test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/val.csv\")        # test set\n",
    "else:\n",
    "    train_triplets = read_triplets_csv(\"./CVUSA_subset/train.csv\")\n",
    "    train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
    "    test_triplets = read_triplets_csv(\"./CVUSA_subset/val.csv\")        # test set\n",
    "\n",
    "train_dataset = CVUSADataset(dataset_dir, train_triplets)\n",
    "val_dataset = CVUSADataset(dataset_dir, val_triplets)\n",
    "test_dataset = CVUSADataset(dataset_dir, test_triplets)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca9a2f",
   "metadata": {},
   "source": [
    "## model  \n",
    "\n",
    "```\n",
    "                                                             ---> Aerial Decoder  \n",
    "                                                           /  \n",
    "Ground Image --> Patch Embedding --> SegFormer Encoder ---  \n",
    "                                                           \\  \n",
    "                                                             ---> Segmentation Decoder  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd99998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualTaskSegFormer(nn.Module):\n",
    "    def __init__(self, pretrained_model=\"nvidia/mit-b1\", num_classes=6):\n",
    "        super().__init__()\n",
    "        # Load pretrained SegFormer\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            pretrained_model,\n",
    "            num_labels=num_classes,\n",
    "            return_dict=False\n",
    "        )\n",
    "        # Remove original classification head\n",
    "        self.segformer.decode_head.classifier = nn.Identity()\n",
    "        \n",
    "        # Get decoder hidden size\n",
    "        decoder_hidden_size = self.segformer.config.decoder_hidden_size\n",
    "        \n",
    "        \n",
    "        # dual-task heads\n",
    "        self.aerial_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_hidden_size, 3, kernel_size=1),\n",
    "            nn.Tanh()  # Output in [-1,1] range\n",
    "        )\n",
    "        \n",
    "        self.seg_head = nn.Conv2d(decoder_hidden_size, num_classes, kernel_size=1)\n",
    "        \n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # Encoder processing\n",
    "        encoder_outputs = self.segformer.segformer(pixel_values)\n",
    "        \n",
    "        # Decoder processing\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        decoder_output = self.segformer.decode_head(encoder_outputs)\n",
    "        \n",
    "        # Dual-task outputs\n",
    "        aerial_output = self.aerial_head(decoder_output)\n",
    "        seg_logits = self.seg_head(decoder_output)\n",
    "        \n",
    "        return aerial_output, seg_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dde92",
   "metadata": {},
   "source": [
    "## lightning wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3c5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningWrapper(pylight.LightningModule):\n",
    "\n",
    "  def __init__(self, device=device, model=DualTaskSegFormer()):\n",
    "    super().__init__()\n",
    "    self.dvc=device\n",
    "    self.model=model\n",
    "    self.criterion_aerial=nn.L1Loss()\n",
    "    self.criterion_segmap=nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    ground, aerial, seg = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    seg = seg.to(self.dvc)\n",
    "    \n",
    "    # Forward pass\n",
    "    aerial_pred, seg_pred = self.model(ground)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
    "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
    "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
    "    self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    ground, aerial, seg = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    seg = seg.to(self.dvc)\n",
    "    \n",
    "    # Forward pass\n",
    "    aerial_pred, seg_pred = self.model(ground)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
    "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
    "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
    "    self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return {\"val_loss\":loss}\n",
    "  \n",
    "\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    ground, aerial, seg = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    seg = seg.to(self.dvc)\n",
    "    \n",
    "    # Forward pass\n",
    "    aerial_pred, seg_pred = self.model(ground)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
    "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
    "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
    "    self.log(\"test_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return {\"test_loss\":loss}\n",
    "\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': self.model.segformer.parameters(), 'lr': 5e-5},\n",
    "        {'params': self.model.seg_head.parameters(), 'lr': 1e-3},\n",
    "        {'params': self.model.aerial_head.parameters(), 'lr': 1e-3}\n",
    "    ], weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "    return {\"optimizer\":optimizer, \"lr_scheduler\":scheduler}\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c498cd2",
   "metadata": {},
   "source": [
    "## Training + Testing (with lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae64b9",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningWrapper(device, model=DualTaskSegFormer()).to(device)\n",
    "\n",
    "if is_colab():\n",
    "    LOG_DIR = \"/content/drive/MyDrive/transformer_logs/\"\n",
    "else:\n",
    "    LOG_DIR = \"./transformer_logs/\"\n",
    "\n",
    "ckpt_path = LOG_DIR + \"checkpoints/\"\n",
    "\n",
    "EPOCHS = 20\n",
    "start_from_epoch = None    # None to start from scratch, or specify epoch number to resume\n",
    "\n",
    "if start_from_epoch:\n",
    "    start_from_path = ckpt_path + f\"best-checkpoint-{start_from_epoch:02d}-*.ckpt\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=ckpt_path,\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:-2f}\",\n",
    "    save_top_k=5,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    save_last=True,\n",
    "    every_n_epochs=2\n",
    ")\n",
    "\n",
    "if not start_from_epoch:\n",
    "    trainer0 = Trainer(\n",
    "        enable_checkpointing=False,\n",
    "        enable_progress_bar=True,\n",
    "        max_epochs=2\n",
    "    )\n",
    "    # Freeze encoder for first 2 epochs\n",
    "    for param in model.segformer.segformer.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    trainer0.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=None)\n",
    "\n",
    "    # Unfreeze encoder\n",
    "    for param in model.segformer.segformer.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    start_from_path = None\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    enable_checkpointing=True,\n",
    "    default_root_dir=LOG_DIR,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=start_from_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc353e",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec56a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "  for i, (ground, aerial, seg) in enumerate(test_loader):\n",
    "    ground = ground.to(device)\n",
    "    aerial = aerial.to(device)\n",
    "    seg = seg.to(device)\n",
    "\n",
    "    aerial_pred, seg_pred = model(ground)\n",
    "\n",
    "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
    "    seg_pred = torchvision.transforms.functional.to_pil_image(seg_pred, mode=None)\n",
    "    \n",
    "    ground = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
    "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
    "    seg = torchvision.transforms.functional.to_pil_image(seg, mode=None)\n",
    "\n",
    "    if i % 443 == 0:\n",
    "      plt.figure(figsize=(15, 10))\n",
    "\n",
    "      plt.subplot(2, 3, 1)\n",
    "      plt.title('Ground view')\n",
    "      plt.imshow(ground)\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.subplot(2, 3, 2)\n",
    "      plt.title('Aerial view')\n",
    "      plt.imshow(aerial)\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.subplot(2, 3, 3)\n",
    "      plt.title('Aerial segmap')\n",
    "      plt.imshow(seg)\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.subplot(2, 3, 4)\n",
    "      plt.title('Aerial view prediction')\n",
    "      plt.imshow(aerial_pred)\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.subplot(2, 3, 5)\n",
    "      plt.title('Aerial segmap prediction')\n",
    "      plt.imshow(seg_pred)\n",
    "      plt.axis('off')\n",
    "\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1208a685",
   "metadata": {},
   "source": [
    "## Training + Testing (no lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3e91b9",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f40e3731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11388a595ed443eb0778db220a33a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\filip\\.cache\\huggingface\\hub\\models--nvidia--mit-b0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a5ea1a98b74ba6863cfa6ee1b813f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m os.makedirs(CHECKPOINT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mDualTaskSegFormer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     17\u001b[39m optimizer = optim.AdamW([\n\u001b[32m     18\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.segformer.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5e-5\u001b[39m},\n\u001b[32m     19\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.seg_head.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-3\u001b[39m},\n\u001b[32m     20\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.aerial_head.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-3\u001b[39m}\n\u001b[32m     21\u001b[39m ], weight_decay=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Learning rate scheduler\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mDualTaskSegFormer.__init__\u001b[39m\u001b[34m(self, pretrained_model, num_classes)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load pretrained SegFormer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mself\u001b[39m.segformer = \u001b[43mSegformerForSemanticSegmentation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Remove original classification head\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.segformer.decode_head.classifier = nn.Identity()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5105\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5102\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5104\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5105\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5106\u001b[39m     )\n\u001b[32m   5108\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5109\u001b[39m prefix = model.base_model_prefix\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:556\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1517\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1518\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1519\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1520\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1521\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1522\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bed2f549a7c43e58d6111d6ee2b7efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "if is_colab():\n",
    "    CHECKPOINT_DIR = \"/content/drive/MyDrive/transformer_checkpoints/\"\n",
    "else:\n",
    "    CHECKPOINT_DIR = \"./transformer_checkpoints/\"\n",
    "\n",
    "SAVE_INTERVAL = 1   # Save checkpoint every SAVE_INTERNAL epochs\n",
    "\n",
    "RESUME = True  # Set True to resume from latest checkpoint\n",
    "\n",
    "# Create checkpoint directory if needed\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = DualTaskSegFormer().to(device)\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.segformer.parameters(), 'lr': 5e-5},\n",
    "    {'params': model.seg_head.parameters(), 'lr': 1e-3},\n",
    "    {'params': model.aerial_head.parameters(), 'lr': 1e-3}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=100,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss functions\n",
    "seg_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "aerial_loss = nn.L1Loss()\n",
    "\n",
    "\n",
    "# ******************************************************************************************* Checkpoint loading function\n",
    "def load_latest_checkpoint():\n",
    "    \"\"\"Loads the latest checkpoint based on epoch number\"\"\"\n",
    "    checkpoint_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch_*.pt\"))\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(\"No checkpoints found. Starting from scratch.\")\n",
    "        return 0, None\n",
    "    \n",
    "    # Extract epoch numbers from filenames\n",
    "    epoch_numbers = []\n",
    "    for f in checkpoint_files:\n",
    "        try:\n",
    "            epoch_num = int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "            epoch_numbers.append(epoch_num)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not epoch_numbers:\n",
    "        print(\"No valid checkpoints found. Starting from scratch.\")\n",
    "        return 0, None\n",
    "    \n",
    "    latest_epoch = max(epoch_numbers)\n",
    "    latest_file = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{latest_epoch}.pt\")\n",
    "    \n",
    "    print(f\"Loading checkpoint: {latest_file}\")\n",
    "    checkpoint = torch.load(latest_file)\n",
    "    \n",
    "    return latest_epoch, checkpoint\n",
    "# *******************************************************************************************\n",
    "\n",
    "\n",
    "# Load checkpoint if resuming\n",
    "if RESUME:\n",
    "    latest_epoch, checkpoint = load_latest_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        START_EPOCH = latest_epoch\n",
    "        print(f\"Resuming training from epoch {START_EPOCH + 1}\")\n",
    "    \n",
    "    else:\n",
    "        START_EPOCH = 0\n",
    "        print(\"Starting training from scratch\")\n",
    "else:\n",
    "    START_EPOCH = 0\n",
    "    print(\"Starting training from scratch (RESUME=False)\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = amp.GradScaler(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40aa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    # Freeze encoder for first 2 epochs if starting from scratch\n",
    "    if epoch < 2 and START_EPOCH == 0:\n",
    "        for param in model.segformer.segformer.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.segformer.segformer.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "\n",
    "    for i, (ground, aerial_true, seg_true) in enumerate(train_loader):\n",
    "        ground = ground.to(device)\n",
    "        aerial_true = aerial_true.to(device)\n",
    "        seg_true = seg_true.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with amp.autocast(device):\n",
    "            aerial_pred, seg_pred = model(ground)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_seg = seg_loss(seg_pred, seg_true)\n",
    "            loss_aerial = aerial_loss(aerial_pred, aerial_true)\n",
    "            total_loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
    "        \n",
    "        # Backpropagation\n",
    "        scaler.scale(total_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        \n",
    "        # Log every 50 batches\n",
    "        if i % 50 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}] | Batch [{i}/{len(train_loader)}] | \"\n",
    "                  f\"Loss: {total_loss.item():.4f} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "\n",
    "    # --------------------------------------------------------------------------- Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for ground, aerial_true, seg_true in val_loader:\n",
    "            ground = ground.to(device)\n",
    "            aerial_true = aerial_true.to(device)\n",
    "            seg_true = seg_true.to(device)\n",
    "            \n",
    "            aerial_pred, seg_pred = model(ground)\n",
    "            loss_seg = seg_loss(seg_pred, seg_true)\n",
    "            loss_aerial = aerial_loss(aerial_pred, aerial_true)\n",
    "            total_val_loss = loss_seg + 0.7 * loss_aerial\n",
    "            val_loss += total_val_loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] completed in {epoch_time:.2f}s | \"\n",
    "          f\"Train Loss: {avg_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    # --------------------------------------------------------------------------- Save checkpoint\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': avg_loss,\n",
    "            'val_loss': avg_val_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498e350d",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for ground, (aerial, _) in test_loader:\n",
    "    ground = ground.to(device)\n",
    "    aerial = aerial.to(device)\n",
    "    aerial_pred = model(ground)\n",
    "\n",
    "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
    "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Satellite Image RGB')\n",
    "    plt.imshow(aerial)     # Original full-size image\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(aerial_pred, cmap='viridis')\n",
    "    plt.title('Satellite prediction')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d927865",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
    "plt.imshow(g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
