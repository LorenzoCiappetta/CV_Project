{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e82572b3",
      "metadata": {
        "id": "e82572b3"
      },
      "source": [
        "# **Transformer model**\n",
        "From query groumd view images generate saellite images (natural and segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e9a4ed",
      "metadata": {
        "id": "14e9a4ed"
      },
      "source": [
        "**TODO**:\n",
        "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
        "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "170e556e",
      "metadata": {
        "id": "170e556e"
      },
      "outputs": [],
      "source": [
        "%pip install torch torchvision transformers scikit-learn pytorch_lightning -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49e9082c",
      "metadata": {
        "id": "49e9082c"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9bcdc708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bcdc708",
        "outputId": "fff7fdcd-a4a3-4f0b-d67d-caeb88042c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "#import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pylight\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "GTOcTL-h1Mkl"
      },
      "id": "GTOcTL-h1Mkl",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "862b2eb7",
      "metadata": {
        "id": "862b2eb7"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d0a046",
      "metadata": {
        "id": "99d0a046"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4e2ed9de",
      "metadata": {
        "id": "4e2ed9de"
      },
      "outputs": [],
      "source": [
        "# TODO: Need to add segmentation of generated images into dataset...\n",
        "\n",
        "class CVUSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ground_dir, aerial_dir, triplet_list, img_size=224, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ground_dir: Directory with all the ground view images\n",
        "            aerial_dir: Directory with all the ground aerial images\n",
        "            split: 'train', 'val' or 'test'\n",
        "            img_size: Size for images, 224x224\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "        \"\"\"\n",
        "\n",
        "        self.ground_dir = ground_dir\n",
        "        self.aerial_dir = aerial_dir\n",
        "        self.triplet_list = triplet_list[:len(triplet_list)//2]\n",
        "        print(len(self.triplet_list))\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Default transforms if none provided\n",
        "        if transform is None:\n",
        "            # For ground view images\n",
        "            self.ground_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial images (we might want different processing)\n",
        "            self.aerial_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # Segmentation transform (nearest neighbor resize)\n",
        "            self.segmentation_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size),\n",
        "                interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                transforms.PILToTensor(),\n",
        "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.ground_transform = transform\n",
        "            self.aerial_transform = transform\n",
        "            self.segmentation_transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        aerial_rel, ground_rel, seg_rel = self.triplet_list[idx]\n",
        "\n",
        "        # Load images\n",
        "        for i in range(3):\n",
        "          try:\n",
        "            ground_img = Image.open(self.ground_dir + ground_rel)\n",
        "            aerial_img = Image.open(self.aerial_dir + aerial_rel)\n",
        "            seg_map = Image.open(self.ground_dir + seg_rel)\n",
        "          except:\n",
        "            if i < 2:\n",
        "              continue\n",
        "            else:\n",
        "              raise Exception(\"Drive isn't Drive-ing\")\n",
        "\n",
        "\n",
        "        # Apply transforms\n",
        "        ground_tensor = self.ground_transform(ground_img)\n",
        "        aerial_tensor = self.aerial_transform(aerial_img)\n",
        "        seg_tensor = self.segmentation_transform(seg_map)  # Shape [H, W]\n",
        "\n",
        "        return ground_tensor, aerial_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "75801a76",
      "metadata": {
        "id": "75801a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674e8d77-f387-4738-a85a-e2fccb3aaac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15101\n",
            "2665\n",
            "4442\n"
          ]
        }
      ],
      "source": [
        "def read_triplets_csv(csv_path):\n",
        "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
        "    triplets = []\n",
        "    with open(csv_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            triplets.append((\n",
        "                parts[0].strip(),  # aerial path\n",
        "                parts[1].strip(),  # ground path\n",
        "                parts[2].strip()   # seg path (ground view segmented map)\n",
        "            ))\n",
        "    return triplets\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    ground_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/bingmap/\"\n",
        "else:\n",
        "    ground_dir = \"./CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"./CV_dataset/CVPR_subset/bingmap/\"\n",
        "\n",
        "\n",
        "train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/train-19zl.csv\")\n",
        "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/val-19zl.csv\")        # test set\n",
        "\n",
        "train_dataset = CVUSADataset(ground_dir, aerial_dir, train_triplets)\n",
        "val_dataset = CVUSADataset(ground_dir, aerial_dir, val_triplets)\n",
        "test_dataset = CVUSADataset(ground_dir, aerial_dir, test_triplets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ee5893c0",
      "metadata": {
        "id": "ee5893c0"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cca9a2f",
      "metadata": {
        "id": "2cca9a2f"
      },
      "source": [
        "## model  \n",
        "\n",
        "```\n",
        "                                                       ---> Aerial Decoder  \n",
        "                                                     /  \n",
        "Ground Image --> Patch Embedding --> ViT Encoder ---  \n",
        "                                                     \\  \n",
        "                                                       ---> Segmentation Decoder  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fdd99998",
      "metadata": {
        "id": "fdd99998"
      },
      "outputs": [],
      "source": [
        "class GroundToAerialTransformer(nn.Module):\n",
        "    def __init__(self, num_seg_classes=7, pretrained=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_seg_classes: Number of segmentation classes\n",
        "            pretrained: Use pretrained ViT weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # ViT Encoder (shared backbone)\n",
        "        model_name = 'google/vit-base-patch16-224-in21k'        # ViT base model, 16x16 patches, 224x224 input size\n",
        "        self.vit_config = ViTConfig.from_pretrained(model_name)\n",
        "        if pretrained:\n",
        "            self.vit = ViTModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.vit = ViTModel(self.vit_config)\n",
        "\n",
        "        # Aerial Image Decoder\n",
        "        self.aerial_decoder = nn.Sequential(\n",
        "            # First upsample to 14x14 (from 197x768)\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 512, kernel_size=2, stride=2),      # convolution\n",
        "            nn.BatchNorm2d(512),        # batch normalization\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 28x28\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 56x56\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample to 224x224\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),     # 3 output channels (RGB)\n",
        "            nn.Tanh()  # Output in [-1, 1] range\n",
        "        )\n",
        "\n",
        "        # Segmentation Head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            # First upsample\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 256, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second upsample\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Third upsample\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(32, num_seg_classes, kernel_size=3, padding=1),       # num_seg_classes output channels (number of segmentation classes)\n",
        "            nn.Softmax(dim=1)  # Multi-class probabilities\n",
        "        )\n",
        "\n",
        "        # Learnable positional embedding for aerial reconstruction\n",
        "        self.aerial_pos_embed = nn.Parameter(torch.zeros(1, 196, self.vit_config.hidden_size))      # 196 = 14x14 (number of patches)\n",
        "        nn.init.trunc_normal_(self.aerial_pos_embed, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode ground image with ViT (process image into patch of tokens)\n",
        "        vit_outputs = self.vit(x)       # Output shape: [batch, 197, hidden_size]\n",
        "\n",
        "        last_hidden_state = vit_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        # remove CLS token for image generation (ViT outputs [CLS] token + 196 patch tokens)\n",
        "        aerial_tokens = last_hidden_state[:, 1:]\n",
        "\n",
        "        # add learned positional embedding for aerial structure\n",
        "        aerial_tokens = aerial_tokens + self.aerial_pos_embed\n",
        "\n",
        "        # Reshape to spatial dimensions (14x14)\n",
        "        batch_size = aerial_tokens.size(0)\n",
        "        aerial_tokens = aerial_tokens.view(batch_size, 14, 14, -1)      # convert 1D sequence into 2D spatial grid. shape becomes: (batch_size, 14, 14, hidden_size)\n",
        "        aerial_tokens = aerial_tokens.permute(0, 3, 1, 2)  # permute shape: (batch_size, hidden_size, 14, 14)\n",
        "\n",
        "        #print(aerial_tokens.shape)\n",
        "        # Decode aerial image\n",
        "        aerial_output = self.aerial_decoder(aerial_tokens)\n",
        "        #print(aerial_output.shape)\n",
        "\n",
        "        # Decode segmentation map\n",
        "        #seg_output = self.segmentation_head(aerial_tokens)\n",
        "\n",
        "\n",
        "        return aerial_output#, seg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lightning Wrapper"
      ],
      "metadata": {
        "id": "3uPQGks-1u8k"
      },
      "id": "3uPQGks-1u8k"
    },
    {
      "cell_type": "code",
      "source": [
        "class LightningWrapper(pylight.LightningModule):\n",
        "  def __init__(self, device, model=GroundToAerialTransformer(num_seg_classes=5)):\n",
        "    super().__init__()\n",
        "    self.dvc=device\n",
        "\n",
        "    self.model=model\n",
        "    self.criterion=nn.L1Loss()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    ground, aerial, _ = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "    # Forward pass\n",
        "    aerial_pred = model(ground)\n",
        "    # Compute loss\n",
        "    loss = self.criterion(aerial_pred, aerial)\n",
        "    self.log(\"train_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "\n",
        "    ground, aerial, _ = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "    # Forward pass\n",
        "    aerial_pred = model(ground)\n",
        "    # Compute loss\n",
        "    loss = self.criterion(aerial_pred, aerial)\n",
        "    self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return {\"val_loss\":loss}\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': self.model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "        {'params': self.model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "        {'params': self.model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "        {'params': self.model.aerial_pos_embed, 'lr': 1e-4}\n",
        "      ], weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "    return {\"optimizer\":optimizer, \"lr_scheduler\":scheduler}\n"
      ],
      "metadata": {
        "id": "y8eHYjnj1usT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098c1d42-158e-43f2-8d99-db6eb4a8c84f"
      },
      "id": "y8eHYjnj1usT",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1208a685",
      "metadata": {
        "id": "1208a685"
      },
      "source": [
        "\n",
        "## training w/ segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40e3731",
      "metadata": {
        "id": "f40e3731"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).cuda()\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true, seg_pred, seg_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    seg_loss = seg_loss_fn(seg_pred, seg_true)\n",
        "    # Weighted combination\n",
        "    return 0.7 * img_loss + 0.3 * seg_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "380e48ef",
      "metadata": {
        "id": "380e48ef"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ground, (aerial, seg) in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial, seg_pred, seg)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e1e3a8",
      "metadata": {
        "id": "86e1e3a8"
      },
      "source": [
        "## Training no segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### No lightning"
      ],
      "metadata": {
        "id": "vi_-hH3n6jjW"
      },
      "id": "vi_-hH3n6jjW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4d9bdb",
      "metadata": {
        "id": "0b4d9bdb"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).to(device)\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    # Weighted combination\n",
        "    return img_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "141dca48",
      "metadata": {
        "id": "141dca48"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, _ in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred = model(ground)\n",
        "        #print(aerial_pred.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial)\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for ground, aerial, _ in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      aerial_pred = model(ground)\n",
        "\n",
        "      loss = total_loss(aerial_pred, aerial)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"content/drive/MyDrive/SavedModels/CV_transformer_1ep\")"
      ],
      "metadata": {
        "id": "4QymIl83pB7U"
      },
      "id": "4QymIl83pB7U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Yes lightning"
      ],
      "metadata": {
        "id": "QjejFRq16nHM"
      },
      "id": "QjejFRq16nHM"
    },
    {
      "cell_type": "code",
      "source": [
        "model = LightningWrapper(device, model=GroundToAerialTransformer(num_seg_classes=5)).to(device)\n",
        "\n",
        "log_path = \"content/drive/MyDrive/SavedModels/CV_transformer/first_try\"\n",
        "ckpt_path = log_path+\"checkpoints\"\n",
        "\n",
        "EPOCHS = 1\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=ckpt_path,\n",
        "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:-2f}\",\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_last=True,\n",
        "    every_n_epochs=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    enable_checkpointing=True,\n",
        "    default_root_dir=log_path,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    enable_progress_bar=True,\n",
        "    max_epochs=1\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318,
          "referenced_widgets": [
            "4807d6cd7cdd49219b27a7d61029bf0f",
            "bdbbed3eefbf438ab567341b28315e36",
            "a96841f26acf43a7af771a7203f12f44",
            "d9c6b95ee15a411b91d37fa73bbacd06",
            "5c8572b0c1c94653b91eb29d485ce7f1",
            "2b852666fc4c4024ab861955f8ba54e1",
            "6bc03d469bf747eaa39cd0de62df0c10",
            "11a1d4d6f975445fbd69370a412664d8",
            "b792ef7231bd46b3bc8116e5451e00f8",
            "930071634f044077be09ca3066f60210",
            "9af4deb37a2546d59ae86b870fcf4e8a",
            "c6cd631fef5f491c899aaef485c474e5",
            "c14d3d918a4c45cb9483620aae036d58",
            "50bfcd5b04be4b888f640691c588e875",
            "8ce3d1f96ef741f494ee5b290123e32d",
            "16298763419b49e3877b7f0c3418273a",
            "6021903ee8cf43f68eb2b4a2b9ab88c1",
            "33265793dead448f95221251cff0c0bc",
            "88d46b027e494c03908859af3feb2ff9",
            "792bb58068a64671bc503cf73d38dce1",
            "df0019e6e64c42019e04d179c31ca7c1",
            "d8e296e59f9d4a89a58b15bb4bed2b35"
          ]
        },
        "id": "GwQgIMRZ6rem",
        "outputId": "3fc43321-2148-4a7e-9224-c6cde9a95f4e"
      },
      "id": "GwQgIMRZ6rem",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name      | Type                      | Params | Mode \n",
            "----------------------------------------------------------------\n",
            "0 | model     | GroundToAerialTransformer | 92.6 M | train\n",
            "1 | criterion | L1Loss                    | 0      | train\n",
            "----------------------------------------------------------------\n",
            "92.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "92.6 M    Total params\n",
            "370.568   Total estimated model params size (MB)\n",
            "32        Modules in train mode\n",
            "215       Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4807d6cd7cdd49219b27a7d61029bf0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6cd631fef5f491c899aaef485c474e5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498e350d",
      "metadata": {
        "id": "498e350d"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf8d9f0",
      "metadata": {
        "id": "1bf8d9f0"
      },
      "outputs": [],
      "source": [
        "model.model.eval()\n",
        "with torch.no_grad():\n",
        "  for ground, (aerial, _) in test_loader:\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    aerial_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "    ground = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Satellite Image RGB')\n",
        "    plt.imshow(aerial)     # Original full-size image\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title('Satellite prediction')\n",
        "    plt.imshow(aerial_pred)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title('Ground')\n",
        "    plt.imshow(ground)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1208a685",
        "vi_-hH3n6jjW"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4807d6cd7cdd49219b27a7d61029bf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdbbed3eefbf438ab567341b28315e36",
              "IPY_MODEL_a96841f26acf43a7af771a7203f12f44",
              "IPY_MODEL_d9c6b95ee15a411b91d37fa73bbacd06"
            ],
            "layout": "IPY_MODEL_5c8572b0c1c94653b91eb29d485ce7f1"
          }
        },
        "bdbbed3eefbf438ab567341b28315e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b852666fc4c4024ab861955f8ba54e1",
            "placeholder": "​",
            "style": "IPY_MODEL_6bc03d469bf747eaa39cd0de62df0c10",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "a96841f26acf43a7af771a7203f12f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11a1d4d6f975445fbd69370a412664d8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b792ef7231bd46b3bc8116e5451e00f8",
            "value": 2
          }
        },
        "d9c6b95ee15a411b91d37fa73bbacd06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_930071634f044077be09ca3066f60210",
            "placeholder": "​",
            "style": "IPY_MODEL_9af4deb37a2546d59ae86b870fcf4e8a",
            "value": " 2/2 [00:24&lt;00:00,  0.08it/s]"
          }
        },
        "5c8572b0c1c94653b91eb29d485ce7f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "2b852666fc4c4024ab861955f8ba54e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bc03d469bf747eaa39cd0de62df0c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11a1d4d6f975445fbd69370a412664d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b792ef7231bd46b3bc8116e5451e00f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "930071634f044077be09ca3066f60210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af4deb37a2546d59ae86b870fcf4e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6cd631fef5f491c899aaef485c474e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c14d3d918a4c45cb9483620aae036d58",
              "IPY_MODEL_50bfcd5b04be4b888f640691c588e875",
              "IPY_MODEL_8ce3d1f96ef741f494ee5b290123e32d"
            ],
            "layout": "IPY_MODEL_16298763419b49e3877b7f0c3418273a"
          }
        },
        "c14d3d918a4c45cb9483620aae036d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6021903ee8cf43f68eb2b4a2b9ab88c1",
            "placeholder": "​",
            "style": "IPY_MODEL_33265793dead448f95221251cff0c0bc",
            "value": "Epoch 0:  28%"
          }
        },
        "50bfcd5b04be4b888f640691c588e875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88d46b027e494c03908859af3feb2ff9",
            "max": 1888,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_792bb58068a64671bc503cf73d38dce1",
            "value": 538
          }
        },
        "8ce3d1f96ef741f494ee5b290123e32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0019e6e64c42019e04d179c31ca7c1",
            "placeholder": "​",
            "style": "IPY_MODEL_d8e296e59f9d4a89a58b15bb4bed2b35",
            "value": " 538/1888 [4:34:07&lt;11:27:52,  0.03it/s, v_num=2, train_loss=0.580]"
          }
        },
        "16298763419b49e3877b7f0c3418273a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "6021903ee8cf43f68eb2b4a2b9ab88c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33265793dead448f95221251cff0c0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d46b027e494c03908859af3feb2ff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "792bb58068a64671bc503cf73d38dce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df0019e6e64c42019e04d179c31ca7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e296e59f9d4a89a58b15bb4bed2b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}