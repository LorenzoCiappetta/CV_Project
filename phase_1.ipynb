{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e82572b3",
      "metadata": {
        "id": "e82572b3"
      },
      "source": [
        "# **Transformer model**\n",
        "From query groumd view images generate saellite images (natural and segmented)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "170e556e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "170e556e",
        "outputId": "602e6d50-9a86-40fb-f052-ffc8dca460e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q torch torchvision transformers scikit-learn pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49e9082c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49e9082c",
        "outputId": "dd9e660c-b8f9-4e76-9b35-99de54ff8a17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab: True\n"
          ]
        }
      ],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "print(\"Running in Colab:\", is_colab())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9bcdc708",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bcdc708",
        "outputId": "0e589e81-9d9b-418b-94b5-475e8fdd3fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import SegformerForSemanticSegmentation\n",
        "\n",
        "import pytorch_lightning as pylight\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from torch import amp\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99d0a046",
      "metadata": {
        "id": "99d0a046"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to map colors in segmentation maps to classes\n",
        "# red: rgb(255,0,0)\t\t      -> [1, 0, 0, 0, 0, 0]\n",
        "# blue: rgb(0,0,255)\t\t    -> [0, 1, 0, 0, 0, 0]\n",
        "# white: rgb(0,0,0)\t\t      -> [0, 0, 1, 0, 0, 0]\n",
        "# green: rgb(0,255,0)\t\t    -> [0, 0, 0, 1, 0, 0]\n",
        "# yellow: rgb(255,255,0)\t\t-> [0, 0, 0, 0, 1, 0]\n",
        "# acqua: rgb(0,255,255)\t\t  -> [0, 0, 0, 0, 0, 1]\n",
        "\n",
        "# Define color-to-class mapping\n",
        "color_map = {\n",
        "    (0, 0, 0): 0,       # white -> class 2\n",
        "    (255, 0, 0): 1,     # red -> class 0\n",
        "    (0, 0, 255): 2,     # blue -> class 1\n",
        "    (0, 255, 0): 3,     # green -> class 3\n",
        "    (255, 255, 0): 4,   # yellow -> class 4\n",
        "    (0, 255, 255): 5,   # cyan -> class 5\n",
        "\n",
        "    # pixels that are not in previous classes are considered 0\n",
        "    (255, 0, 255): 0,\n",
        "    (255, 255, 255): 0\n",
        "}\n",
        "\n",
        "def rgb_to_label(rgb_image):\n",
        "    \"\"\"Convert RGB image to class index tensor\"\"\"\n",
        "\n",
        "    # Convert to numpy array\n",
        "    np_image = np.array(rgb_image)\n",
        "    # manage different colors\n",
        "    np_image = np.where(np_image < 128, 0, 255)\n",
        "    # Create empty label map\n",
        "    label_map = np.zeros((np_image.shape[0], np_image.shape[1]), dtype=np.int64)\n",
        "\n",
        "    # Map colors to class indices\n",
        "    for color, class_idx in color_map.items():\n",
        "        # Find pixels matching this RGB color\n",
        "        mask = (np_image == np.array(color)).all(axis=-1)\n",
        "        label_map[mask] = class_idx\n",
        "\n",
        "    # Convert to tensor\n",
        "    label_tensor = torch.from_numpy(label_map)\n",
        "    # Convert to one-hot\n",
        "    one_hot = F.one_hot(label_tensor, num_classes=6).float()\n",
        "\n",
        "    #return torch.from_numpy(label_map)\n",
        "    return torch.from_numpy(np.array(one_hot.transpose(2, 0)))\n",
        "\n",
        "\n",
        "# path = '/content/drive/MyDrive/CVUSA_subset/polarmap/segmap/output0000008.png'\n",
        "# img = Image.open(path)\n",
        "# img_np = np.array(img)\n",
        "# print(img_np.shape)\n",
        "\n",
        "# res = rgb_to_label(img)\n",
        "# print(res)\n",
        "# print(res.shape)"
      ],
      "metadata": {
        "id": "9mCJvbJhXOXd"
      },
      "id": "9mCJvbJhXOXd",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "4e2ed9de",
      "metadata": {
        "id": "4e2ed9de"
      },
      "outputs": [],
      "source": [
        "class CVUSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_dir, triplet_list, img_size=224, transform=None):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.triplet_list = triplet_list\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Default transforms if none provided\n",
        "        if transform is None:\n",
        "            # For ground view images\n",
        "            self.ground_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial images\n",
        "            self.aerial_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial segmentation maps\n",
        "            self.segmentation_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                transforms.Lambda(rgb_to_label),\n",
        "                #transforms.PILToTensor(),\n",
        "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.ground_transform = transform\n",
        "            self.aerial_transform = transform\n",
        "            self.segmentation_transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ground_rel, aerial_rel, seg_rel = self.triplet_list[idx]\n",
        "\n",
        "        max_retries = 3\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Load images\n",
        "                ground_img = Image.open(self.dataset_dir + ground_rel).convert('RGB')\n",
        "                aerial_img = Image.open(self.dataset_dir + aerial_rel).convert('RGB')\n",
        "                seg_map = Image.open(self.dataset_dir + seg_rel)\n",
        "\n",
        "                # Break if successful\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading images for index {idx}: {e}\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    raise e\n",
        "\n",
        "\n",
        "        # Apply transforms\n",
        "        ground_tensor = self.ground_transform(ground_img)\n",
        "        aerial_tensor = self.aerial_transform(aerial_img)\n",
        "        seg_tensor = self.segmentation_transform(seg_map)\n",
        "\n",
        "        return ground_tensor, aerial_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "75801a76",
      "metadata": {
        "id": "75801a76"
      },
      "outputs": [],
      "source": [
        "def read_triplets_csv(csv_path):\n",
        "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
        "    triplets = []\n",
        "    with open(csv_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            triplets.append((\n",
        "                parts[0].strip(),  # aerial path\n",
        "                parts[1].strip(),  # ground path\n",
        "                parts[2].strip()   # seg path (ground view segmented map)\n",
        "            ))\n",
        "    return triplets\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    dataset_dir = \"/content/drive/MyDrive/CVUSA_subset/\"\n",
        "else:\n",
        "    dataset_dir = \"./CVUSA_subset\"\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/train.csv\")\n",
        "    train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "    test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/val.csv\")        # test set\n",
        "else:\n",
        "    train_triplets = read_triplets_csv(\"./CVUSA_subset/train.csv\")\n",
        "    train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "    test_triplets = read_triplets_csv(\"./CVUSA_subset/val.csv\")        # test set\n",
        "\n",
        "train_dataset = CVUSADataset(dataset_dir, train_triplets)\n",
        "val_dataset = CVUSADataset(dataset_dir, val_triplets)\n",
        "test_dataset = CVUSADataset(dataset_dir, test_triplets)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cca9a2f",
      "metadata": {
        "id": "2cca9a2f"
      },
      "source": [
        "## model  \n",
        "\n",
        "```\n",
        "                                                             ---> Aerial Decoder  \n",
        "                                                           /  \n",
        "Ground Image --> Patch Embedding --> SegFormer Encoder ---  \n",
        "                                                           \\  \n",
        "                                                             ---> Segmentation Decoder  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "fdd99998",
      "metadata": {
        "id": "fdd99998"
      },
      "outputs": [],
      "source": [
        "class DualTaskSegFormer(nn.Module):\n",
        "    def __init__(self, pretrained_model=\"nvidia/mit-b1\", num_classes=6):\n",
        "        super().__init__()\n",
        "        # Load pretrained SegFormer\n",
        "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
        "            pretrained_model,\n",
        "            num_labels=num_classes,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=False\n",
        "        )\n",
        "        # Remove original classification head\n",
        "        self.segformer.decode_head.classifier = nn.Identity()\n",
        "\n",
        "        # Get decoder hidden size\n",
        "        decoder_hidden_size = self.segformer.config.decoder_hidden_size   # 256\n",
        "\n",
        "        # dual-task heads\n",
        "        self.aerial_head = nn.Sequential(\n",
        "            nn.ConvTranspose2d(decoder_hidden_size, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        self.seg_head = nn.Sequential(\n",
        "            nn.ConvTranspose2d(decoder_hidden_size, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 6, kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Encoder processing\n",
        "        encoder_outputs = self.segformer.segformer(pixel_values, output_hidden_states=True, return_dict=False)\n",
        "\n",
        "        # Extract multi-scale features [stage1, stage2, stage3, stage4]\n",
        "        hidden_states = encoder_outputs[1]  # Tuple of 4 feature maps\n",
        "\n",
        "        # Decoder processing\n",
        "        #decoder_output = self.segformer.decode_head(encoder_outputs)\n",
        "        decoder_output = self.segformer.decode_head(hidden_states)\n",
        "\n",
        "        # Dual-task outputs\n",
        "        aerial_output = self.aerial_head(decoder_output)\n",
        "        seg_logits = self.seg_head(decoder_output)\n",
        "\n",
        "        return aerial_output, seg_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "706dde92",
      "metadata": {
        "id": "706dde92"
      },
      "source": [
        "## lightning wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "8e3c5ce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e3c5ce2",
        "outputId": "46231a84-aae7-4d08-a6da-aacf2dea7315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b1 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "class LightningWrapper(pylight.LightningModule):\n",
        "\n",
        "  def __init__(self, device=device, model=DualTaskSegFormer()):\n",
        "    super().__init__()\n",
        "    self.dvc=device\n",
        "    self.model=model\n",
        "    self.criterion_aerial=nn.L1Loss()\n",
        "    self.criterion_segmap=nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    ground, aerial, seg = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    seg = seg.to(self.dvc)\n",
        "    seg = seg.to(dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    aerial_pred, seg_pred = self.model(ground)\n",
        "\n",
        "    # print(f\"seg shape: {seg.shape}\\nseg_pred shape: {seg_pred.shape}\\naerial_pred shape: {aerial_pred.shape}\")        # DEBUG\n",
        "    # print(f\"type seg: {seg.dtype}\\ntype seg_pred: {seg_pred.dtype}\")        # DEBUG\n",
        "    # print(f\"aerial: {aerial}\")        # DEBUG\n",
        "\n",
        "    # Compute loss\n",
        "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
        "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
        "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
        "    self.log(\"train_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    ground, aerial, seg = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    seg = seg.to(self.dvc)\n",
        "    seg = seg.to(dtype=torch.float32)\n",
        "\n",
        "    # Forward pass\n",
        "    aerial_pred, seg_pred = self.model(ground)\n",
        "\n",
        "    # print(f\"seg shape: {seg.shape}\\nseg_pred shape: {seg_pred.shape}\\naerial_pred shape: {aerial_pred.shape}\")        # DEBUG\n",
        "    # print(f\"type seg: {seg.dtype}\\ntype seg_pred: {seg_pred.dtype}\")        # DEBUG\n",
        "    # print(f\"aerial: {aerial}\")        # DEBUG\n",
        "\n",
        "    # Compute loss\n",
        "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
        "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
        "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
        "    self.log(\"val_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return {\"val_loss\":loss}\n",
        "\n",
        "\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    ground, aerial, seg = batch\n",
        "    ground = ground.to(self.dvc)\n",
        "    aerial = aerial.to(self.dvc)\n",
        "    seg = seg.to(self.dvc)\n",
        "\n",
        "    # Forward pass\n",
        "    aerial_pred, seg_pred = self.model(ground)\n",
        "\n",
        "    # Compute loss\n",
        "    loss_seg = self.criterion_segmap(seg_pred, seg)\n",
        "    loss_aerial = self.criterion_aerial(aerial_pred, aerial)\n",
        "    loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
        "    self.log(\"test_loss\", loss, prog_bar=True)\n",
        "\n",
        "    return {\"test_loss\":loss}\n",
        "\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    optimizer = optim.AdamW([\n",
        "        {'params': self.model.segformer.parameters(), 'lr': 5e-5},\n",
        "        {'params': self.model.seg_head.parameters(), 'lr': 1e-3},\n",
        "        {'params': self.model.aerial_head.parameters(), 'lr': 1e-3}\n",
        "    ], weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
        "    return {\"optimizer\":optimizer, \"lr_scheduler\":scheduler}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c498cd2",
      "metadata": {
        "id": "6c498cd2"
      },
      "source": [
        "## Training + Testing (with lightning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cae64b9",
      "metadata": {
        "id": "5cae64b9"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "e8c5090a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825,
          "referenced_widgets": [
            "f8111a61641f4072b269b15f130d39bb",
            "442a92b345ca44c4a3e10fcf68f4b370",
            "844a57eb334e49eb80f7221e8ff1ab16",
            "753f70bb7481450bb18147cfbe4c6c7b",
            "611b44f055574fac8886e9671164dfd4",
            "187e6594f78d4adab7e9a96adaf76c18",
            "de7fa580d70a42fe81addbc430de7bc6",
            "2a1b324980a442dba1481f5bc9d4752f",
            "74392bc2f31645c5a0e12c8b7e15c073",
            "82810fc1a1554e86b3c8efdf7abb0ef4",
            "cb00a61aaefa43b7b9e7e1512484aac6",
            "db70c2b7a1c14b1b89ed85116536e3af",
            "2be70502dc79460cbc4e14eb5575cb99",
            "3e3fa551b0c74252b86a5cadeb768449",
            "78efd3d5fba94fe5ae0f92b24eec232e",
            "46daf6c757ac4186be4f76432f329365",
            "dbc25a159992409aa6c8a3cd5131a43e",
            "c9265ffbc4d7420a869d4cf037d34586",
            "bd1d84577de042c483806934ba03bdd3",
            "f8c04887cb1b4106ab8bd9867abc0b74",
            "28196d1839b24c599e91c8872d464fdb",
            "7b72653330e1480ba0566c7c181eeb10"
          ]
        },
        "id": "e8c5090a",
        "outputId": "e13da004-96bf-4c88-c9dd-26668e5241a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b1 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name             | Type              | Params | Mode \n",
            "---------------------------------------------------------------\n",
            "0 | model            | DualTaskSegFormer | 13.8 M | train\n",
            "1 | criterion_aerial | L1Loss            | 0      | train\n",
            "2 | criterion_segmap | CrossEntropyLoss  | 0      | train\n",
            "---------------------------------------------------------------\n",
            "659 K     Trainable params\n",
            "13.2 M    Non-trainable params\n",
            "13.8 M    Total params\n",
            "55.243    Total estimated model params size (MB)\n",
            "12        Modules in train mode\n",
            "212       Modules in eval mode\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8111a61641f4072b269b15f130d39bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db70c2b7a1c14b1b89ed85116536e3af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exit' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    598\u001b[0m         )\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mdataloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;31m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# this will run only when no pre-fetching was done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fetchers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Sequential\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/combined_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-93-1341510191.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# Load images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mground_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mground_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0maerial_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maerial_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3516\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-103-3062600784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrainer0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Unfreeze encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_SubprocessScriptLauncher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_sigkill_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
          ]
        }
      ],
      "source": [
        "model = LightningWrapper(device, model=DualTaskSegFormer()).to(device)\n",
        "\n",
        "if is_colab():\n",
        "    LOG_DIR = \"/content/drive/MyDrive/transformer_logs/\"\n",
        "else:\n",
        "    LOG_DIR = \"./transformer_logs/\"\n",
        "\n",
        "ckpt_path = LOG_DIR + \"checkpoints/\"\n",
        "\n",
        "EPOCHS = 20\n",
        "start_from_epoch = None    # None to start from scratch, or specify epoch number to resume\n",
        "\n",
        "if start_from_epoch:\n",
        "    start_from_path = ckpt_path + f\"best-checkpoint-{start_from_epoch:02d}-*.ckpt\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=ckpt_path,\n",
        "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:-2f}\",\n",
        "    save_top_k=5,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        "    save_last=True,\n",
        "    every_n_epochs=2\n",
        ")\n",
        "\n",
        "if not start_from_epoch:\n",
        "    trainer0 = Trainer(\n",
        "        enable_checkpointing=False,\n",
        "        enable_progress_bar=True,\n",
        "        max_epochs=2\n",
        "    )\n",
        "    # Freeze encoder for first 2 epochs\n",
        "    for param in model.model.segformer.segformer.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    trainer0.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=None)\n",
        "\n",
        "    # Unfreeze encoder\n",
        "    for param in model.model.segformer.segformer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    start_from_path = None\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    enable_checkpointing=True,\n",
        "    default_root_dir=LOG_DIR,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    enable_progress_bar=True,\n",
        "    max_epochs=EPOCHS\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=start_from_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fc353e",
      "metadata": {
        "id": "63fc353e"
      },
      "source": [
        "### testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec56a6a",
      "metadata": {
        "id": "aec56a6a"
      },
      "outputs": [],
      "source": [
        "model.model.eval()\n",
        "with torch.no_grad():\n",
        "  for i, (ground, aerial, seg) in enumerate(test_loader):\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    seg = seg.to(device)\n",
        "\n",
        "    aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    seg_pred = torchvision.transforms.functional.to_pil_image(seg_pred, mode=None)\n",
        "\n",
        "    ground = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "    seg = torchvision.transforms.functional.to_pil_image(seg, mode=None)\n",
        "\n",
        "    if i % 443 == 0:\n",
        "      plt.figure(figsize=(15, 10))\n",
        "\n",
        "      plt.subplot(2, 3, 1)\n",
        "      plt.title('Ground view')\n",
        "      plt.imshow(ground)\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.subplot(2, 3, 2)\n",
        "      plt.title('Aerial view')\n",
        "      plt.imshow(aerial)\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.subplot(2, 3, 3)\n",
        "      plt.title('Aerial segmap')\n",
        "      plt.imshow(seg)\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.subplot(2, 3, 4)\n",
        "      plt.title('Aerial view prediction')\n",
        "      plt.imshow(aerial_pred)\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.subplot(2, 3, 5)\n",
        "      plt.title('Aerial segmap prediction')\n",
        "      plt.imshow(seg_pred)\n",
        "      plt.axis('off')\n",
        "\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1208a685",
      "metadata": {
        "id": "1208a685"
      },
      "source": [
        "## Training + Testing (no lightning)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a3e91b9",
      "metadata": {
        "id": "3a3e91b9"
      },
      "source": [
        "### training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f40e3731",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b11388a595ed443eb0778db220a33a95",
            "14a5ea1a98b74ba6863cfa6ee1b813f0",
            "4bed2f549a7c43e58d6111d6ee2b7efd"
          ]
        },
        "id": "f40e3731",
        "outputId": "c95b0931-0d9f-4b62-a4ca-99b0b5ff7b4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b11388a595ed443eb0778db220a33a95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\filip\\.cache\\huggingface\\hub\\models--nvidia--mit-b0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14a5ea1a98b74ba6863cfa6ee1b813f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "ValueError",
          "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m os.makedirs(CHECKPOINT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m model = \u001b[43mDualTaskSegFormer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     17\u001b[39m optimizer = optim.AdamW([\n\u001b[32m     18\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.segformer.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m5e-5\u001b[39m},\n\u001b[32m     19\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.seg_head.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-3\u001b[39m},\n\u001b[32m     20\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: model.aerial_head.parameters(), \u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1e-3\u001b[39m}\n\u001b[32m     21\u001b[39m ], weight_decay=\u001b[32m1e-4\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Learning rate scheduler\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mDualTaskSegFormer.__init__\u001b[39m\u001b[34m(self, pretrained_model, num_classes)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load pretrained SegFormer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mself\u001b[39m.segformer = \u001b[43mSegformerForSemanticSegmentation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Remove original classification head\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mself\u001b[39m.segformer.decode_head.classifier = nn.Identity()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4839\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4830\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4832\u001b[39m     (\n\u001b[32m   4833\u001b[39m         model,\n\u001b[32m   4834\u001b[39m         missing_keys,\n\u001b[32m   4835\u001b[39m         unexpected_keys,\n\u001b[32m   4836\u001b[39m         mismatched_keys,\n\u001b[32m   4837\u001b[39m         offload_index,\n\u001b[32m   4838\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4839\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4845\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4848\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4857\u001b[39m \u001b[38;5;66;03m# record tp degree the model sharded to\u001b[39;00m\n\u001b[32m   4858\u001b[39m model._tp_size = tp_size\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:5105\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5102\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(state_dict.keys())\n\u001b[32m   5103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5104\u001b[39m     original_checkpoint_keys = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m5105\u001b[39m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m.keys()\n\u001b[32m   5106\u001b[39m     )\n\u001b[32m   5108\u001b[39m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[32m   5109\u001b[39m prefix = model.base_model_prefix\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:556\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\filip\\OneDrive\\Desktop\\Computer Vision\\progetto\\CV_Project\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1517\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1518\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1519\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1520\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1521\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1522\u001b[39m         )\n",
            "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bed2f549a7c43e58d6111d6ee2b7efd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "if is_colab():\n",
        "    CHECKPOINT_DIR = \"/content/drive/MyDrive/transformer_checkpoints/\"\n",
        "else:\n",
        "    CHECKPOINT_DIR = \"./transformer_checkpoints/\"\n",
        "\n",
        "SAVE_INTERVAL = 1   # Save checkpoint every SAVE_INTERNAL epochs\n",
        "\n",
        "RESUME = True  # Set True to resume from latest checkpoint\n",
        "\n",
        "# Create checkpoint directory if needed\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = DualTaskSegFormer().to(device)\n",
        "optimizer = optim.AdamW([\n",
        "    {'params': model.segformer.parameters(), 'lr': 5e-5},\n",
        "    {'params': model.seg_head.parameters(), 'lr': 1e-3},\n",
        "    {'params': model.aerial_head.parameters(), 'lr': 1e-3}\n",
        "], weight_decay=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# Loss functions\n",
        "seg_loss = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "aerial_loss = nn.L1Loss()\n",
        "\n",
        "\n",
        "# ******************************************************************************************* Checkpoint loading function\n",
        "def load_latest_checkpoint():\n",
        "    \"\"\"Loads the latest checkpoint based on epoch number\"\"\"\n",
        "    checkpoint_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"checkpoint_epoch_*.pt\"))\n",
        "\n",
        "    if not checkpoint_files:\n",
        "        print(\"No checkpoints found. Starting from scratch.\")\n",
        "        return 0, None\n",
        "\n",
        "    # Extract epoch numbers from filenames\n",
        "    epoch_numbers = []\n",
        "    for f in checkpoint_files:\n",
        "        try:\n",
        "            epoch_num = int(f.split(\"_\")[-1].split(\".\")[0])\n",
        "            epoch_numbers.append(epoch_num)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not epoch_numbers:\n",
        "        print(\"No valid checkpoints found. Starting from scratch.\")\n",
        "        return 0, None\n",
        "\n",
        "    latest_epoch = max(epoch_numbers)\n",
        "    latest_file = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{latest_epoch}.pt\")\n",
        "\n",
        "    print(f\"Loading checkpoint: {latest_file}\")\n",
        "    checkpoint = torch.load(latest_file)\n",
        "\n",
        "    return latest_epoch, checkpoint\n",
        "# *******************************************************************************************\n",
        "\n",
        "\n",
        "# Load checkpoint if resuming\n",
        "if RESUME:\n",
        "    latest_epoch, checkpoint = load_latest_checkpoint()\n",
        "\n",
        "    if checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        START_EPOCH = latest_epoch\n",
        "        print(f\"Resuming training from epoch {START_EPOCH + 1}\")\n",
        "\n",
        "    else:\n",
        "        START_EPOCH = 0\n",
        "        print(\"Starting training from scratch\")\n",
        "else:\n",
        "    START_EPOCH = 0\n",
        "    print(\"Starting training from scratch (RESUME=False)\")\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = amp.GradScaler(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b40aa91d",
      "metadata": {
        "id": "b40aa91d"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for epoch in range(START_EPOCH, EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Freeze encoder for first 2 epochs if starting from scratch\n",
        "    if epoch < 2 and START_EPOCH == 0:\n",
        "        for param in model.segformer.segformer.parameters():\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        for param in model.segformer.segformer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "    for i, (ground, aerial_true, seg_true) in enumerate(train_loader):\n",
        "        ground = ground.to(device)\n",
        "        aerial_true = aerial_true.to(device)\n",
        "        seg_true = seg_true.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with amp.autocast(device):\n",
        "            aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "            # Compute losses\n",
        "            loss_seg = seg_loss(seg_pred, seg_true)\n",
        "            loss_aerial = aerial_loss(aerial_pred, aerial_true)\n",
        "            total_loss = loss_seg + 0.7 * loss_aerial  # Weighted sum, favor segmentation accuracy\n",
        "\n",
        "        # Backpropagation\n",
        "        scaler.scale(total_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += total_loss.item()\n",
        "\n",
        "        # Log every 50 batches\n",
        "        if i % 50 == 0:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch [{epoch+1}/{EPOCHS}] | Batch [{i}/{len(train_loader)}] | \"\n",
        "                  f\"Loss: {total_loss.item():.4f} | LR: {current_lr:.2e}\")\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------- Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        for ground, aerial_true, seg_true in val_loader:\n",
        "            ground = ground.to(device)\n",
        "            aerial_true = aerial_true.to(device)\n",
        "            seg_true = seg_true.to(device)\n",
        "\n",
        "            aerial_pred, seg_pred = model(ground)\n",
        "            loss_seg = seg_loss(seg_pred, seg_true)\n",
        "            loss_aerial = aerial_loss(aerial_pred, aerial_true)\n",
        "            total_val_loss = loss_seg + 0.7 * loss_aerial\n",
        "            val_loss += total_val_loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] completed in {epoch_time:.2f}s | \"\n",
        "          f\"Train Loss: {avg_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------------------------- Save checkpoint\n",
        "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_loss': avg_loss,\n",
        "            'val_loss': avg_val_loss\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "498e350d",
      "metadata": {
        "id": "498e350d"
      },
      "source": [
        "### testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf8d9f0",
      "metadata": {
        "id": "1bf8d9f0"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for ground, (aerial, _) in test_loader:\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    aerial_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Satellite Image RGB')\n",
        "    plt.imshow(aerial)     # Original full-size image\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(aerial_pred, cmap='viridis')\n",
        "    plt.title('Satellite prediction')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d927865",
      "metadata": {
        "id": "8d927865"
      },
      "outputs": [],
      "source": [
        "g = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
        "plt.imshow(g)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8111a61641f4072b269b15f130d39bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_442a92b345ca44c4a3e10fcf68f4b370",
              "IPY_MODEL_844a57eb334e49eb80f7221e8ff1ab16",
              "IPY_MODEL_753f70bb7481450bb18147cfbe4c6c7b"
            ],
            "layout": "IPY_MODEL_611b44f055574fac8886e9671164dfd4"
          }
        },
        "442a92b345ca44c4a3e10fcf68f4b370": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187e6594f78d4adab7e9a96adaf76c18",
            "placeholder": "​",
            "style": "IPY_MODEL_de7fa580d70a42fe81addbc430de7bc6",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "844a57eb334e49eb80f7221e8ff1ab16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a1b324980a442dba1481f5bc9d4752f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74392bc2f31645c5a0e12c8b7e15c073",
            "value": 2
          }
        },
        "753f70bb7481450bb18147cfbe4c6c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82810fc1a1554e86b3c8efdf7abb0ef4",
            "placeholder": "​",
            "style": "IPY_MODEL_cb00a61aaefa43b7b9e7e1512484aac6",
            "value": " 2/2 [00:09&lt;00:00,  0.22it/s]"
          }
        },
        "611b44f055574fac8886e9671164dfd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "187e6594f78d4adab7e9a96adaf76c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de7fa580d70a42fe81addbc430de7bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a1b324980a442dba1481f5bc9d4752f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74392bc2f31645c5a0e12c8b7e15c073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82810fc1a1554e86b3c8efdf7abb0ef4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb00a61aaefa43b7b9e7e1512484aac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db70c2b7a1c14b1b89ed85116536e3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2be70502dc79460cbc4e14eb5575cb99",
              "IPY_MODEL_3e3fa551b0c74252b86a5cadeb768449",
              "IPY_MODEL_78efd3d5fba94fe5ae0f92b24eec232e"
            ],
            "layout": "IPY_MODEL_46daf6c757ac4186be4f76432f329365"
          }
        },
        "2be70502dc79460cbc4e14eb5575cb99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbc25a159992409aa6c8a3cd5131a43e",
            "placeholder": "​",
            "style": "IPY_MODEL_c9265ffbc4d7420a869d4cf037d34586",
            "value": "Epoch 0:   1%"
          }
        },
        "3e3fa551b0c74252b86a5cadeb768449": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd1d84577de042c483806934ba03bdd3",
            "max": 707,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8c04887cb1b4106ab8bd9867abc0b74",
            "value": 5
          }
        },
        "78efd3d5fba94fe5ae0f92b24eec232e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28196d1839b24c599e91c8872d464fdb",
            "placeholder": "​",
            "style": "IPY_MODEL_7b72653330e1480ba0566c7c181eeb10",
            "value": " 5/707 [00:48&lt;1:52:27,  0.10it/s, v_num=20, train_loss=2.230]"
          }
        },
        "46daf6c757ac4186be4f76432f329365": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "dbc25a159992409aa6c8a3cd5131a43e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9265ffbc4d7420a869d4cf037d34586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd1d84577de042c483806934ba03bdd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c04887cb1b4106ab8bd9867abc0b74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28196d1839b24c599e91c8872d464fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b72653330e1480ba0566c7c181eeb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}