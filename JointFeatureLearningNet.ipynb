{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e464ef64",
   "metadata": {},
   "source": [
    "# **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision transformers scikit-learn pytorch_lightning\n",
    "%pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de193a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import pytorch_lightning as pylight\n",
    "import torch.optim as optim\n",
    "\n",
    "import glob\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import os\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06acc95",
   "metadata": {},
   "source": [
    "# **GLOBALS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac28198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------- DATA\n",
    "# standard ImageNet normalization\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# ----------------------------------------- Paths\n",
    "NUM_TRANSFORMER_MODEL = 3\n",
    "\n",
    "DATASET_DIR = \"/content/drive/MyDrive/CVUSA_subset/\"\n",
    "SYNT_DATASET_DIR = f\"/content/drive/MyDrive/CVUSA_subset/generated_images/generated_images_model_{NUM_TRANSFORMER_MODEL}/\"\n",
    "LOG_DIR = \"/content/drive/MyDrive/JFLN_logs/\"\n",
    "\n",
    "CKPT_PATH = LOG_DIR + \"checkpoints_1/\"\n",
    "\n",
    "\n",
    "# ----------------------------------------- TRAIN\n",
    "LR_vgg = 1e-4\n",
    "LR_fnn = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "GRADIENT_CLIP = 0.8\n",
    "\n",
    "EPOCHS = 25\n",
    "\n",
    "START_FROM_EPOCH = None     # None to start training from scratch, or specify an epoch number to resume training (-1 to use last checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e0dc9",
   "metadata": {},
   "source": [
    "# **UTILS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define color-to-class mapping\n",
    "color_map = {\n",
    "    (255, 255, 255): 0,   # white -> class 0\n",
    "    (255, 0, 0): 1,       # red -> class 1\n",
    "    (0, 0, 255): 2,       # blue -> class 2\n",
    "    (0, 255, 0): 3,       # green -> class 3\n",
    "    (255, 255, 0): 4,     # yellow -> class 4\n",
    "    (0, 255, 255): 5,     # cyan -> class 5\n",
    "\n",
    "    # pixels that are not in previous classes are considered 0\n",
    "    (255, 0, 255): 0,\n",
    "    (255, 255, 255): 0\n",
    "}\n",
    "\n",
    "\n",
    "def rgb_to_label(rgb_image):\n",
    "    \"\"\"Convert RGB image to class index tensor\"\"\"\n",
    "\n",
    "    # Convert to numpy array\n",
    "    np_image = np.array(rgb_image)\n",
    "    # manage different colors\n",
    "    np_image = np.where(np_image < 128, 0, 255)\n",
    "    # Create empty label map\n",
    "    label_map = np.zeros((np_image.shape[0], np_image.shape[1]), dtype=np.int64)\n",
    "\n",
    "    # Map colors to class indices\n",
    "    for color, class_idx in color_map.items():\n",
    "        color_arr = np.array(color)\n",
    "        mask = (np_image == color_arr).all(axis=-1)\n",
    "        label_map[mask] = class_idx\n",
    "\n",
    "    return torch.from_numpy(label_map).unsqueeze(0)\n",
    "\n",
    "\n",
    "# path = '/content/drive/MyDrive/CVUSA_subset/polarmap/segmap/output0000008.png'\n",
    "# img = Image.open(path)\n",
    "# img_np = np.array(img)\n",
    "# print(img_np.shape)     # (128, 512, 3)\n",
    "\n",
    "# res = rgb_to_label(img)\n",
    "# print(res)\n",
    "# print(res.shape)\n",
    "\n",
    "\n",
    "def to_onehot(x, num_classes=6):\n",
    "    # x is a LongTensor [1×H×W] with values 0..5\n",
    "    x = x.long().squeeze(0)                      # [H×W]\n",
    "    onehot = F.one_hot(x, num_classes=num_classes)  # [H×W×6]\n",
    "    return onehot.permute(2, 0, 1).float()           # [6×H×W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "769f478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_triplets_csv(csv_path):\n",
    "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
    "    triplets = []\n",
    "    with open(csv_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            triplets.append((\n",
    "                parts[0].strip(),  # ground path\n",
    "                parts[1].strip(),  # aerial path\n",
    "                parts[2].strip()   # seg path\n",
    "            ))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "# train_triplets = read_triplets_csv(\"./CVUSA_subset/train.csv\")\n",
    "# print(train_triplets[:1])  # Print first 5 triplets for verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55240d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss with weighted soft margin\n",
    "def WeightedSoftMarginTripletLoss(a, p, n, margin=1.0):\n",
    "    # a: anchor, p: positive, n: negative\n",
    "    pos_dist = F.pairwise_distance(a, p, p=2)\n",
    "    neg_dist = F.pairwise_distance(a, n, p=2)\n",
    "    return torch.log1p(torch.exp(pos_dist - neg_dist + margin)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624acba3",
   "metadata": {},
   "source": [
    "# **DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc64461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "class CVUSADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, synt_dataset_dir, triplet_list, img_size=224, add_dir = \"\", train=False):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.synt_dataset_dir = synt_dataset_dir\n",
    "        self.triplet_list = triplet_list\n",
    "        self.img_size = img_size\n",
    "        self.add_dir = add_dir\n",
    "\n",
    "        # transformations with data augmentation\n",
    "        if train:\n",
    "            # For ground view images\n",
    "            self.ground_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomPerspective(distortion_scale=0.15, p=0.4),    # random perspective\n",
    "                transforms.RandomApply([transforms.ColorJitter(0.3, 0.2, 0.1, 0.05)], p=0.5),   # augment brightness, contrast, saturation and hue\n",
    "                transforms.RandomGrayscale(p=0.1),    # convert image to grayscale\n",
    "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),   # blur\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "            # For aerial images\n",
    "            self.aerial_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.RandomApply([transforms.ColorJitter(0.3, 0.2, 0.1, 0.05)], p=0.5),   # augment brightness, contrast, saturation and hue\n",
    "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),   # blur\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "        else:\n",
    "            # For ground view images\n",
    "            self.ground_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
    "            ])\n",
    "            # For aerial images\n",
    "            self.aerial_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=MEAN, std=STD)\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "        # For aerial segmentation maps\n",
    "        self.segmentation_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.Lambda(rgb_to_label),\n",
    "            transforms.Lambda(lambda x: to_onehot(x, num_classes=6))\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplet_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ground_rel, aerial_rel, _ = self.triplet_list[idx]\n",
    "\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Load images\n",
    "                ground_img = Image.open(self.dataset_dir + ground_rel).convert('RGB')\n",
    "                aerial_img = Image.open(self.dataset_dir + aerial_rel).convert('RGB')\n",
    "\n",
    "                img_id_number = ground_rel.split(\"/\")[-1].split(\".\")[0]\n",
    "                # Load synthetic images\n",
    "                synth_img = Image.open((self.synt_dataset_dir + self.add_dir + \"/aerial_predictions/seg_pred_\" + img_id_number + \".png\").strip()).convert('RGB')\n",
    "                seg_synth_img = Image.open((self.synt_dataset_dir + self.add_dir + \"/segmap_predictions/aerial_pred_\" + img_id_number + \".png\").strip()).convert('RGB')\n",
    "\n",
    "                # Break if successful\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading images for index {idx}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise e\n",
    "\n",
    "\n",
    "        # Apply transforms\n",
    "        ground_tensor = self.ground_transform(ground_img).to(dtype=torch.float32)\n",
    "        aerial_tensor = self.aerial_transform(aerial_img).to(dtype=torch.float32)\n",
    "        synth_tensor = self.aerial_transform(synth_img).to(dtype=torch.float32)\n",
    "        seg_synth_tensor = self.segmentation_transform(seg_synth_img).to(dtype=torch.float32)\n",
    "\n",
    "        return ground_tensor, aerial_tensor, synth_tensor, seg_synth_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/train.csv\")\n",
    "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
    "test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CVUSA_subset/val.csv\")        # test set\n",
    "\n",
    "train_dataset = CVUSADataset(DATASET_DIR, SYNT_DATASET_DIR, train_triplets, add_dir=\"train_set\", train=True)\n",
    "val_dataset = CVUSADataset(DATASET_DIR, SYNT_DATASET_DIR, val_triplets, add_dir=\"val_set\")\n",
    "test_dataset = CVUSADataset(DATASET_DIR, SYNT_DATASET_DIR, test_triplets, add_dir=\"test_set\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0accefc",
   "metadata": {},
   "source": [
    "# **NETWORK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd9070",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb68241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointFeatureLearningNet(nn.Module):\n",
    "    def __init__(self, embed_dim=512, freeze_vgg_layers=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vgg_ground = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        self.vgg_aerial = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        self.vgg_seg = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # Modify size of input for segmaps: 6 channels (one-hot)\n",
    "        self.vgg_seg.features[0] = nn.Conv2d(6, 64, kernel_size=3, padding=1)\n",
    "        # Initiate weights\n",
    "        nn.init.kaiming_normal_(self.vgg_seg.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "        # Freezing initial layers for finetuning\n",
    "        for backbone in (self.vgg_ground.features, self.vgg_aerial.features, self.vgg_seg.features):\n",
    "            for layer in backbone[:freeze_vgg_layers]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = False\n",
    "        \n",
    "        self.reduce = nn.Conv2d(512, 128, kernel_size=1)\n",
    "        nn.init.kaiming_normal_(self.reduce.weight, nonlinearity='relu')\n",
    "\n",
    "        # Define embedding heads for multi-scale features\n",
    "        self.fc_ground = nn.Linear(3 * 128 * 7 * 7, embed_dim)\n",
    "        self.fc_aerial = nn.Linear(3 * 128 * 7 * 7, embed_dim)\n",
    "        self.fc_seg    = nn.Linear(3 * 128 * 7 * 7, embed_dim)\n",
    "    \n",
    "\n",
    "    def _extract_multiscale(self, backbone, x):\n",
    "        # Collect activations after features[16], [23], [30] (conv6,7,8 in VGG16)\n",
    "        layers = [16, 23, 30]\n",
    "        feats = []\n",
    "        for idx, layer in enumerate(backbone.features):\n",
    "            x = layer(x)\n",
    "            if idx in layers:\n",
    "                x = F.relu(self.reduce(x))      # B×128×H×W\n",
    "                feats.append(x.view(x.size(0), -1))\n",
    "        return torch.cat(feats, dim=1)  # B x (3*C*H*W)\n",
    "\n",
    "\n",
    "    def forward(self, ground_img, aerial_img, synth_img, seg_synth_img):\n",
    "        # Multi-scale features\n",
    "        f_ground_ms = self._extract_multiscale(self.vgg_ground, ground_img)\n",
    "        f_aerial_ms = self._extract_multiscale(self.vgg_aerial, aerial_img)\n",
    "        f_synth_aerial_ms= self._extract_multiscale(self.vgg_aerial, synth_img)\n",
    "        f_synth_seg_ms = self._extract_multiscale(self.vgg_seg, seg_synth_img)\n",
    "\n",
    "        # Embeddings\n",
    "        f_ground = self.fc_ground(f_ground_ms)\n",
    "        f_aerial = self.fc_aerial(f_aerial_ms)\n",
    "        f_synth_aerial = self.fc_aerial(f_synth_aerial_ms)  # share head with real aerial\n",
    "        f_synth_seg = self.fc_seg(f_synth_seg_ms)\n",
    "\n",
    "        # L2 normalize embeddings\n",
    "        f_ground  = F.normalize(f_ground, dim=1)\n",
    "        f_aerial  = F.normalize(f_aerial, dim=1)\n",
    "        f_synth_aerial = F.normalize(f_synth_aerial, dim=1)\n",
    "        f_synth_seg  = F.normalize(f_synth_seg, dim=1)\n",
    "\n",
    "        return f_ground, f_aerial, f_synth_aerial, f_synth_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6998e4",
   "metadata": {},
   "source": [
    "## lightning wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningWrapper(pylight.LightningModule):\n",
    "\n",
    "  def __init__(self, device=device, model=JointFeatureLearningNet(), l1_weight=10, l2_weight=1, l3_weight=1):\n",
    "    super().__init__()\n",
    "    self.dvc = device\n",
    "    self.model = model\n",
    "\n",
    "    self.criterion = WeightedSoftMarginTripletLoss()\n",
    "    self.l1_weight = l1_weight\n",
    "    self.l2_weight = l2_weight\n",
    "    self.l3_weight = l3_weight\n",
    "\n",
    "\n",
    "  def forward(self, ground_view, candidate_aerial, synthetic_aerial, synthetic_seg_aerial):\n",
    "    return self.model(ground_view, candidate_aerial, synthetic_aerial, synthetic_seg_aerial)\n",
    "\n",
    "\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    ground, aerial, synth_aerial, synth_seg_aerial = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    synth_aerial = synth_aerial.to(self.dvc)\n",
    "    synth_seg_aerial = synth_seg_aerial.to(self.dvc)\n",
    "\n",
    "    # Forward pass\n",
    "    f_ground, f_aerial, f_synth_aerial, f_synth_seg = self.model(ground, aerial, synth_aerial, synth_seg_aerial)\n",
    "\n",
    "    # Find closest (hard) negative in batch\n",
    "    # Euclidean distance matrix\n",
    "    dists = torch.cdist(f_ground, f_aerial, p=2)\n",
    "    # Mask out the diagonals\n",
    "    dists.fill_diagonal_(float('inf'))\n",
    "    # For each anchor i, get hardest negative index\n",
    "    neg_idx = torch.argmin(dists, dim=1)  # shape [B]\n",
    "    # negative aerial features\n",
    "    f_aerial_neg = f_aerial[neg_idx]\n",
    "\n",
    "    # Compute loss\n",
    "    L1 = self.criterion(f_ground, f_aerial, f_aerial_neg)\n",
    "    L2 = self.criterion(f_synth_aerial, f_aerial, f_aerial_neg)\n",
    "    L3 = self.criterion(f_synth_seg, f_aerial, f_aerial_neg)\n",
    "    loss = self.l1_weight * L1 + self.l2_weight * L2 + self.l3_weight * L3\n",
    "\n",
    "    self.log(\"loss 1\", L1, prog_bar=True)\n",
    "    self.log(\"loss 2\", L2, prog_bar=True)\n",
    "    self.log(\"loss 3\", L3, prog_bar=True)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    ground, aerial, synth_aerial, synth_seg_aerial = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    synth_aerial = synth_aerial.to(self.dvc)\n",
    "    synth_seg_aerial = synth_seg_aerial.to(self.dvc)\n",
    "\n",
    "    # Forward pass\n",
    "    f_ground, f_aerial, f_synth_aerial, f_synth_seg = self.model(ground, aerial, synth_aerial, synth_seg_aerial)\n",
    "\n",
    "    # Find closest (hard) negative in batch\n",
    "    # Euclidean distance matrix\n",
    "    dists = torch.cdist(f_ground, f_aerial, p=2)\n",
    "    # Mask out the diagonals\n",
    "    dists.fill_diagonal_(float('inf'))\n",
    "    # For each anchor i, get hardest negative index\n",
    "    neg_idx = torch.argmin(dists, dim=1)  # shape [B]\n",
    "    # negative aerial features\n",
    "    f_aerial_neg = f_aerial[neg_idx]\n",
    "\n",
    "    # Compute loss\n",
    "    L1 = self.criterion(f_ground, f_aerial, f_aerial_neg)\n",
    "    L2 = self.criterion(f_synth_aerial, f_aerial, f_aerial_neg)\n",
    "    L3 = self.criterion(f_synth_seg, f_aerial, f_aerial_neg)\n",
    "    loss = self.l1_weight * L1 + self.l2_weight * L2 + self.l3_weight * L3\n",
    "\n",
    "    self.log(\"loss 1\", L1, prog_bar=True)\n",
    "    self.log(\"loss 2\", L2, prog_bar=True)\n",
    "    self.log(\"loss 3\", L3, prog_bar=True)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    ground, aerial, synth_aerial, synth_seg_aerial = batch\n",
    "    ground = ground.to(self.dvc)\n",
    "    aerial = aerial.to(self.dvc)\n",
    "    synth_aerial = synth_aerial.to(self.dvc)\n",
    "    synth_seg_aerial = synth_seg_aerial.to(self.dvc)\n",
    "\n",
    "    # Forward pass\n",
    "    f_ground, f_aerial, f_synth_aerial, f_synth_seg = self.model(ground, aerial, synth_aerial, synth_seg_aerial)\n",
    "\n",
    "    # Find closest (hard) negative in batch\n",
    "    # Euclidean distance matrix\n",
    "    dists = torch.cdist(f_ground, f_aerial, p=2)\n",
    "    # Mask out the diagonals\n",
    "    dists.fill_diagonal_(float('inf'))\n",
    "    # For each anchor i, get hardest negative index\n",
    "    neg_idx = torch.argmin(dists, dim=1)  # shape [B]\n",
    "    # negative aerial features\n",
    "    f_aerial_neg = f_aerial[neg_idx]\n",
    "\n",
    "    # Compute loss\n",
    "    L1 = self.criterion(f_ground, f_aerial, f_aerial_neg)\n",
    "    L2 = self.criterion(f_synth_aerial, f_aerial, f_aerial_neg)\n",
    "    L3 = self.criterion(f_synth_seg, f_aerial, f_aerial_neg)\n",
    "    loss = self.l1_weight * L1 + self.l2_weight * L2 + self.l3_weight * L3\n",
    "\n",
    "    self.log(\"loss 1\", L1, prog_bar=True)\n",
    "    self.log(\"loss 2\", L2, prog_bar=True)\n",
    "    self.log(\"loss 3\", L3, prog_bar=True)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': self.model.vgg_ground.parameters(), 'lr': LR_vgg},\n",
    "        {'params': self.model.vgg_aerial.parameters(), 'lr': LR_vgg},\n",
    "        {'params': self.model.vgg_seg.parameters(), 'lr': LR_vgg},\n",
    "        {'params': self.model.fc_ground.parameters(), 'lr': LR_fnn},\n",
    "        {'params': self.model.fc_aerial.parameters(), 'lr': LR_fnn},\n",
    "        {'params': self.model.fc_seg.parameters(), 'lr': LR_fnn}\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    scheduler = {\n",
    "        'scheduler': optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[LR_vgg, LR_vgg, LR_vgg, LR_fnn, LR_fnn, LR_fnn],  # Matches optimizer groups\n",
    "            total_steps=len(train_loader)*EPOCHS,\n",
    "            pct_start=0.3,\n",
    "            div_factor=10,\n",
    "            final_div_factor=100\n",
    "        ),\n",
    "        'interval': 'step'\n",
    "    }\n",
    "    \n",
    "    return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ec5f9",
   "metadata": {},
   "source": [
    "# **TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845df85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if START_FROM_EPOCH is not None:\n",
    "    if START_FROM_EPOCH == -1:    # start from last checkpoint\n",
    "        pattern = CKPT_PATH + \"last.ckpt\"\n",
    "        matching = glob.glob(pattern)\n",
    "        if len(matching) == 0:\n",
    "            raise FileNotFoundError(f\"No checkpoint file matches: {pattern}\")\n",
    "        start_from_path = matching[0]\n",
    "\n",
    "    else:\n",
    "        pattern = CKPT_PATH + f\"best-checkpoint-epoch={START_FROM_EPOCH:02d}-*.ckpt\"\n",
    "        matching = glob.glob(pattern)\n",
    "        if len(matching) == 0:\n",
    "            raise FileNotFoundError(f\"No checkpoint file matches: {pattern}\")\n",
    "        start_from_path = matching[0]\n",
    "else:\n",
    "    start_from_path = None\n",
    "\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=LOG_DIR,\n",
    "    name=\"lightning_logs\",\n",
    ")\n",
    "\n",
    "# ___________________________________________________________________________________________ Load model\n",
    "\n",
    "model = LightningWrapper(device, model=JointFeatureLearningNet(), l1_weight=10, l2_weight=1, l3_weight=1)\n",
    "\n",
    "\n",
    "# ___________________________________________________________________________________________ Callbacks\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=CKPT_PATH,\n",
    "    filename=\"best-checkpoint-{epoch:02d}-{val_loss:-2f}\",\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=True,\n",
    "    save_top_k=6,\n",
    "    every_n_epochs=2,\n",
    "    save_last=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=7,          # Stop after patience epochs without improvement\n",
    "    min_delta=0.0001,    # Minimum change to qualify as improvement\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    checkpoint_callback,\n",
    "    early_stop_callback\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ___________________________________________________________________________________________ Training phase 1\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    enable_checkpointing=True,\n",
    "    default_root_dir=LOG_DIR,\n",
    "    callbacks=callbacks_list,\n",
    "    enable_progress_bar=True,\n",
    "    max_epochs=EPOCHS,\n",
    "    gradient_clip_val=GRADIENT_CLIP,\n",
    "    accumulate_grad_batches=2,  # Effective batch size = BATCH_SIZE * 2\n",
    "    precision='16-mixed'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader, ckpt_path=start_from_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f54674",
   "metadata": {},
   "source": [
    "# **TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_to_load = -1     # number of epoch of file to load. -1 to take last checkpoint\n",
    "\n",
    "if epoch_to_load == -1:\n",
    "    ckpt_to_load = CKPT_PATH + \"last.ckpt\"\n",
    "else:\n",
    "    pattern = os.path.join(CKPT_PATH, f\"*epoch={epoch_to_load:02d}-*.ckpt\")\n",
    "    matching_files = glob.glob(pattern)\n",
    "    if not matching_files:\n",
    "        raise FileNotFoundError(f\"No checkpoint file found with epoch {epoch_to_load}.\")\n",
    "    ckpt_to_load = matching_files[0]\n",
    "\n",
    "print(f\"Loading model checkpoint {ckpt_to_load} ...\")\n",
    "\n",
    "model = LightningWrapper.load_from_checkpoint(\n",
    "    ckpt_to_load,\n",
    "    device=device,\n",
    "    model=JointFeatureLearningNet(),\n",
    "    l1_weight=10,\n",
    "    l2_weight=1,\n",
    "    l3_weight=1\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "testing_logger = TensorBoardLogger(\n",
    "    save_dir=LOG_DIR,\n",
    "    name=\"testing_lightning_logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=testing_logger,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "test_results = trainer.test(\n",
    "    model,\n",
    "    test_dataloaders=test_loader\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n=== Test Results ===\")\n",
    "for key, val in test_results[0].items():\n",
    "    print(f\"{key}: {val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
