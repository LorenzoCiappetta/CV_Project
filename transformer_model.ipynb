{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4261e7ce",
   "metadata": {},
   "source": [
    "# **Transformer model**\n",
    "From query groumd view images generate saellite images (natural and segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877e253",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
    "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26b9d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision transformers scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ea87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e2bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filippo/Scrivania/Computer Vision/CV_Project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "#import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428b0422",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30d127",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e39bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVUSADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ground_dir, aerial_dir, triplet_list, img_size=224, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ground_dir: Directory with all the ground view images\n",
    "            aerial_dir: Directory with all the ground aerial images\n",
    "            split: 'train', 'val' or 'test'\n",
    "            img_size: Size for images, 224x224\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ground_dir = ground_dir\n",
    "        self.aerial_dir = aerial_dir\n",
    "        self.triplet_list = triplet_list\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Default transforms if none provided\n",
    "        if transform is None:\n",
    "            # For ground view images\n",
    "            self.ground_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
    "            ])\n",
    "            # For aerial images (we might want different processing)\n",
    "            self.aerial_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
    "            ])\n",
    "            # Segmentation transform (nearest neighbor resize)\n",
    "            self.segmentation_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size), \n",
    "                interpolation=transforms.InterpolationMode.NEAREST),\n",
    "                transforms.PILToTensor(),\n",
    "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
    "            ])\n",
    "        else:\n",
    "            self.ground_transform = transform\n",
    "            self.aerial_transform = transform\n",
    "            self.segmentation_transform = transform\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triplet_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        aerial_rel, ground_rel, seg_rel = self.triplet_list[idx]\n",
    "\n",
    "        # Load images\n",
    "        ground_img = Image.open(self.ground_dir + ground_rel)\n",
    "        aerial_img = Image.open(self.aerial_dir + aerial_rel)\n",
    "        seg_map = Image.open(self.ground_dir + seg_rel)\n",
    "\n",
    "        # Apply transforms\n",
    "        ground_tensor = self.ground_transform(ground_img)\n",
    "        aerial_tensor = self.aerial_transform(aerial_img)\n",
    "        seg_tensor = self.segmentation_transform(seg_map)  # Shape [H, W]\n",
    "\n",
    "        return ground_tensor, aerial_tensor, seg_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97369ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_triplets_csv(csv_path):\n",
    "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
    "    triplets = []\n",
    "    with open(csv_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(',')\n",
    "            triplets.append((\n",
    "                parts[0].strip(),  # aerial path\n",
    "                parts[1].strip(),  # ground path\n",
    "                parts[2].strip()   # seg path (ground view segmented map)\n",
    "            ))\n",
    "    return triplets\n",
    "\n",
    "\n",
    "if is_colab():\n",
    "    ground_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/streetview/\"\n",
    "    aerial_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/bingmap/\"\n",
    "else:\n",
    "    ground_dir = \"./CV_dataset/CVPR_subset/streetview/\"\n",
    "    aerial_dir = \"./CV_dataset/CVPR_subset/bingmap/\"\n",
    "\n",
    "\n",
    "train_triplets = read_triplets_csv(\"./CV_dataset/CVPR_subset/splits/splits/train-19zl.csv\")\n",
    "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
    "test_triplets = read_triplets_csv(\"./CV_dataset/CVPR_subset/splits/splits/val-19zl.csv\")        # test set\n",
    "\n",
    "train_dataset = CVUSADataset(ground_dir, aerial_dir, train_triplets)\n",
    "val_dataset = CVUSADataset(ground_dir, aerial_dir, val_triplets)\n",
    "test_dataset = CVUSADataset(ground_dir, aerial_dir, test_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24aeab",
   "metadata": {},
   "source": [
    "## model  \n",
    "\n",
    "```\n",
    "                                                       ---> Aerial Decoder  \n",
    "                                                     /  \n",
    "Ground Image --> Patch Embedding --> ViT Encoder ---  \n",
    "                                                     \\  \n",
    "                                                       ---> Segmentation Decoder  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundToAerialTransformer(nn.Module):\n",
    "    def __init__(self, num_seg_classes=7, pretrained=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_seg_classes: Number of segmentation classes\n",
    "            pretrained: Use pretrained ViT weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # ViT Encoder (shared backbone)\n",
    "        model_name = 'google/vit-base-patch16-224-in21k'        # ViT base model, 16x16 patches, 224x224 input size\n",
    "        self.vit_config = ViTConfig.from_pretrained(model_name)\n",
    "        if pretrained:\n",
    "            self.vit = ViTModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.vit = ViTModel(self.vit_config)\n",
    "        \n",
    "        # Aerial Image Decoder\n",
    "        self.aerial_decoder = nn.Sequential(\n",
    "            # First upsample to 14x14 (from 197x768)\n",
    "            nn.ConvTranspose2d(self.vit_config.hidden_size, 512, kernel_size=4, stride=2),      # convolution\n",
    "            nn.BatchNorm2d(512),        # batch normalization\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Upsample to 28x28\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Upsample to 56x56\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final upsample to 224x224\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),     # 3 output channels (RGB)\n",
    "            nn.Tanh()  # Output in [-1, 1] range\n",
    "        )\n",
    "        \n",
    "        # Segmentation Head\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            # First upsample\n",
    "            nn.ConvTranspose2d(self.vit_config.hidden_size, 256, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second upsample\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Third upsample\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final upsample\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Conv2d(32, num_seg_classes, kernel_size=3, padding=1),       # num_seg_classes output channels (number of segmentation classes)\n",
    "            nn.Softmax(dim=1)  # Multi-class probabilities\n",
    "        )\n",
    "        \n",
    "        # Learnable positional embedding for aerial reconstruction\n",
    "        self.aerial_pos_embed = nn.Parameter(torch.zeros(1, 196, self.vit_config.hidden_size))      # 196 = 14x14 (number of patches)\n",
    "        nn.init.trunc_normal_(self.aerial_pos_embed, std=0.02)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode ground image with ViT (process image into patch of tokens)\n",
    "        vit_outputs = self.vit(x)       # Output shape: [batch, 197, hidden_size]\n",
    "\n",
    "        last_hidden_state = vit_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "        \n",
    "        # remove CLS token for image generation (ViT outputs [CLS] token + 196 patch tokens)\n",
    "        aerial_tokens = last_hidden_state[:, 1:]\n",
    "\n",
    "        # add learned positional embedding for aerial structure\n",
    "        aerial_tokens = aerial_tokens + self.aerial_pos_embed\n",
    "        \n",
    "        # Reshape to spatial dimensions (14x14)\n",
    "        batch_size = aerial_tokens.size(0)\n",
    "        aerial_tokens = aerial_tokens.view(batch_size, 14, 14, -1)      # convert 1D sequence into 2D spatial grid. shape becomes: (batch_size, 14, 14, hidden_size)\n",
    "        aerial_tokens = aerial_tokens.permute(0, 3, 1, 2)  # permute shape: (batch_size, hidden_size, 14, 14)\n",
    "        \n",
    "        # Decode aerial image\n",
    "        aerial_output = self.aerial_decoder(aerial_tokens)\n",
    "        \n",
    "        # Decode segmentation map\n",
    "        seg_output = self.segmentation_head(aerial_tokens)\n",
    "        \n",
    "        return aerial_output, seg_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92440dfe",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b21423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = GroundToAerialTransformer(num_seg_classes=5).cuda()\n",
    "\n",
    "# Loss functions\n",
    "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
    "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
    "\n",
    "# Combined loss with weighting\n",
    "def total_loss(aerial_pred, aerial_true, seg_pred, seg_true):\n",
    "    # Image reconstruction loss\n",
    "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
    "    # Segmentation loss\n",
    "    seg_loss = seg_loss_fn(seg_pred, seg_true)\n",
    "    # Weighted combination\n",
    "    return 0.7 * img_loss + 0.3 * seg_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
    "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler (adjust learning rate during training)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98987bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for ground, (aerial, seg) in dataloader:\n",
    "        ground = ground.to(device)\n",
    "        aerial = aerial.to(device)\n",
    "        seg = seg.to(device)  # Assuming seg is preprocessed\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()       # resets gradients from previous batch\n",
    "        aerial_pred, seg_pred = model(ground)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = total_loss(aerial_pred, aerial, seg_pred, seg)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()         # computes gradients via backpropagation\n",
    "        optimizer.step()        # updates weights using gradients\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "# Main training\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    scheduler.step()    # adjusts learning rate after each epoch\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
