{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4261e7ce",
      "metadata": {
        "id": "4261e7ce"
      },
      "source": [
        "# **Transformer model**\n",
        "From query groumd view images generate saellite images (natural and segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b877e253",
      "metadata": {
        "id": "b877e253"
      },
      "source": [
        "**TODO**:\n",
        "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
        "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a26b9d6c",
      "metadata": {
        "id": "a26b9d6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91ac95f-2c93-4bd4-d597-10fb093c7df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision transformers scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1ea87d6",
      "metadata": {
        "id": "a1ea87d6"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "756e2bc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "756e2bc5",
        "outputId": "d43015da-6efa-4950-b4e1-c612f2136051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "#import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "428b0422",
      "metadata": {
        "id": "428b0422"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a30d127",
      "metadata": {
        "id": "6a30d127"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e39bbf",
      "metadata": {
        "id": "90e39bbf"
      },
      "outputs": [],
      "source": [
        "# TODO: Need to add segmentation of generated images into dataset...\n",
        "\n",
        "class CVUSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ground_dir, aerial_dir, triplet_list, img_size=224, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ground_dir: Directory with all the ground view images\n",
        "            aerial_dir: Directory with all the ground aerial images\n",
        "            split: 'train', 'val' or 'test'\n",
        "            img_size: Size for images, 224x224\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "        \"\"\"\n",
        "\n",
        "        self.ground_dir = ground_dir\n",
        "        self.aerial_dir = aerial_dir\n",
        "        self.triplet_list = triplet_list\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Default transforms if none provided\n",
        "        if transform is None:\n",
        "            # For ground view images\n",
        "            self.ground_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial images (we might want different processing)\n",
        "            self.aerial_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # Segmentation transform (nearest neighbor resize)\n",
        "            self.segmentation_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size),\n",
        "                interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                transforms.PILToTensor(),\n",
        "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.ground_transform = transform\n",
        "            self.aerial_transform = transform\n",
        "            self.segmentation_transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        aerial_rel, ground_rel, seg_rel = self.triplet_list[idx]\n",
        "\n",
        "        # Load images\n",
        "        ground_img = Image.open(self.ground_dir + ground_rel)\n",
        "        aerial_img = Image.open(self.aerial_dir + aerial_rel)\n",
        "        seg_map = Image.open(self.ground_dir + seg_rel)\n",
        "\n",
        "        # Apply transforms\n",
        "        ground_tensor = self.ground_transform(ground_img)\n",
        "        aerial_tensor = self.aerial_transform(aerial_img)\n",
        "        seg_tensor = self.segmentation_transform(seg_map)  # Shape [H, W]\n",
        "\n",
        "        return ground_tensor, aerial_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97369ebc",
      "metadata": {
        "id": "97369ebc"
      },
      "outputs": [],
      "source": [
        "def read_triplets_csv(csv_path):\n",
        "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
        "    triplets = []\n",
        "    with open(csv_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            triplets.append((\n",
        "                parts[0].strip(),  # aerial path\n",
        "                parts[1].strip(),  # ground path\n",
        "                parts[2].strip()   # seg path (ground view segmented map)\n",
        "            ))\n",
        "    return triplets\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    ground_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/bingmap/\"\n",
        "else:\n",
        "    ground_dir = \"./CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"./CV_dataset/CVPR_subset/bingmap/\"\n",
        "\n",
        "\n",
        "train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/train-19zl.csv\")\n",
        "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/val-19zl.csv\")        # test set\n",
        "\n",
        "train_dataset = CVUSADataset(ground_dir, aerial_dir, train_triplets)\n",
        "val_dataset = CVUSADataset(ground_dir, aerial_dir, val_triplets)\n",
        "test_dataset = CVUSADataset(ground_dir, aerial_dir, test_triplets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "v3UgFB1kAhBZ"
      },
      "id": "v3UgFB1kAhBZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "df24aeab",
      "metadata": {
        "id": "df24aeab"
      },
      "source": [
        "## model  \n",
        "\n",
        "```\n",
        "                                                       ---> Aerial Decoder  \n",
        "                                                     /  \n",
        "Ground Image --> Patch Embedding --> ViT Encoder ---  \n",
        "                                                     \\  \n",
        "                                                       ---> Segmentation Decoder  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ee57cd",
      "metadata": {
        "id": "a6ee57cd"
      },
      "outputs": [],
      "source": [
        "class GroundToAerialTransformer(nn.Module):\n",
        "    def __init__(self, num_seg_classes=7, pretrained=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_seg_classes: Number of segmentation classes\n",
        "            pretrained: Use pretrained ViT weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # ViT Encoder (shared backbone)\n",
        "        model_name = 'google/vit-base-patch16-224-in21k'        # ViT base model, 16x16 patches, 224x224 input size\n",
        "        self.vit_config = ViTConfig.from_pretrained(model_name)\n",
        "        if pretrained:\n",
        "            self.vit = ViTModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.vit = ViTModel(self.vit_config)\n",
        "\n",
        "        # Aerial Image Decoder\n",
        "        self.aerial_decoder = nn.Sequential(\n",
        "            # First upsample to 14x14 (from 197x768)\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 512, kernel_size=2, stride=2),      # convolution\n",
        "            nn.BatchNorm2d(512),        # batch normalization\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 28x28\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 56x56\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample to 224x224\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),     # 3 output channels (RGB)\n",
        "            nn.Tanh()  # Output in [-1, 1] range\n",
        "        )\n",
        "\n",
        "        # Segmentation Head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            # First upsample\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 256, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second upsample\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Third upsample\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(32, num_seg_classes, kernel_size=3, padding=1),       # num_seg_classes output channels (number of segmentation classes)\n",
        "            nn.Softmax(dim=1)  # Multi-class probabilities\n",
        "        )\n",
        "\n",
        "        # Learnable positional embedding for aerial reconstruction\n",
        "        self.aerial_pos_embed = nn.Parameter(torch.zeros(1, 196, self.vit_config.hidden_size))      # 196 = 14x14 (number of patches)\n",
        "        nn.init.trunc_normal_(self.aerial_pos_embed, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode ground image with ViT (process image into patch of tokens)\n",
        "        vit_outputs = self.vit(x)       # Output shape: [batch, 197, hidden_size]\n",
        "\n",
        "        last_hidden_state = vit_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        # remove CLS token for image generation (ViT outputs [CLS] token + 196 patch tokens)\n",
        "        aerial_tokens = last_hidden_state[:, 1:]\n",
        "\n",
        "        # add learned positional embedding for aerial structure\n",
        "        aerial_tokens = aerial_tokens + self.aerial_pos_embed\n",
        "\n",
        "        # Reshape to spatial dimensions (14x14)\n",
        "        batch_size = aerial_tokens.size(0)\n",
        "        aerial_tokens = aerial_tokens.view(batch_size, 14, 14, -1)      # convert 1D sequence into 2D spatial grid. shape becomes: (batch_size, 14, 14, hidden_size)\n",
        "        aerial_tokens = aerial_tokens.permute(0, 3, 1, 2)  # permute shape: (batch_size, hidden_size, 14, 14)\n",
        "\n",
        "        #print(aerial_tokens.shape)\n",
        "        # Decode aerial image\n",
        "        aerial_output = self.aerial_decoder(aerial_tokens)\n",
        "        #print(aerial_output.shape)\n",
        "\n",
        "        # Decode segmentation map\n",
        "        #seg_output = self.segmentation_head(aerial_tokens)\n",
        "\n",
        "\n",
        "        return aerial_output#, seg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92440dfe",
      "metadata": {
        "id": "92440dfe"
      },
      "source": [
        "\n",
        "## training w/ segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b21423",
      "metadata": {
        "id": "b2b21423"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).cuda()\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true, seg_pred, seg_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    seg_loss = seg_loss_fn(seg_pred, seg_true)\n",
        "    # Weighted combination\n",
        "    return 0.7 * img_loss + 0.3 * seg_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98987bb4",
      "metadata": {
        "id": "98987bb4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ground, (aerial, seg) in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial, seg_pred, seg)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training no segmentation"
      ],
      "metadata": {
        "id": "3QwZksLzdhul"
      },
      "id": "3QwZksLzdhul"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).to(device)\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    # Weighted combination\n",
        "    return img_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189,
          "referenced_widgets": [
            "bac5fbf5a00b4529b113aa1e496d3cda",
            "b99738f91bed4fadbc0ac380acba6199",
            "f42690cd3bc849f1891ea23bcd70ec8a",
            "df50676124374335826a8a2abb1940bc",
            "edfe68800a4b45499550e278322be115",
            "78bc58f6143243f2a184b18b3ba551c6",
            "4b3269f2a5af4292818fedb673ee1a7e",
            "e4bf1ac970a045f59f1b92a7434d4421",
            "830129a60a8b454d8b64a543d5f6f4fa",
            "3914bf7bf5604da7b082d6412861a7b6",
            "98cbfc471e12416caba8e7b5162c61ae",
            "d4d4f316ff7c4154a42465eeb79fe024",
            "1e287b5cd4fc40bc8129f51b5a5c75d9",
            "1a899b71ba4544cd94c733742fcd5f06",
            "47a537182fdf4cd4ab53d616447d36e4",
            "7e78b0e120044946bc2b341d4fa60bb3",
            "8a6bad5f838a4d9aad0f5dd3b10eaa1b",
            "96b56669297e4418ac29b46ff1b5242a",
            "bcda66cbe8ed446c9c8a19fa4c722496",
            "28386b597ba941cc8065513d7aa540c3",
            "33200016f1f14680b7a8d598633e4e1e",
            "27965d019bd64a9e8812ec74faa02884"
          ]
        },
        "id": "OQXLQ8oLdoi4",
        "outputId": "54113953-fc23-47dc-a04a-b168f0f19d84"
      },
      "id": "OQXLQ8oLdoi4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bac5fbf5a00b4529b113aa1e496d3cda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4d4f316ff7c4154a42465eeb79fe024"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, _ in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred = model(ground)\n",
        "        #print(aerial_pred.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial)\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for ground, aerial, _ in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      aerial_pred = model(ground)\n",
        "\n",
        "      loss = total_loss(aerial_pred, aerial)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ],
      "metadata": {
        "id": "4vEfA5TrdrEi"
      },
      "id": "4vEfA5TrdrEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "9IpzLyOog-jX"
      },
      "id": "9IpzLyOog-jX"
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for ground, (aerial, _) in test_loader:\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    aerial_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Satellite Image RGB')\n",
        "    plt.imshow(aerial)     # Original full-size image\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(aerial_pred, cmap='viridis')\n",
        "    plt.title('Satellite prediction')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    break"
      ],
      "metadata": {
        "id": "Cl1vAKgqhCny"
      },
      "id": "Cl1vAKgqhCny",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torchvision.transforms.functional.to_pil_image(ground, mode=None)\n",
        "plt.imshow(g)"
      ],
      "metadata": {
        "id": "nvCZGduBAoxS"
      },
      "id": "nvCZGduBAoxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Second Model**\n",
        "(Based on current understanding of task)"
      ],
      "metadata": {
        "id": "BAYcDP2PHM-C"
      },
      "id": "BAYcDP2PHM-C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Half"
      ],
      "metadata": {
        "id": "HZkhPctO10SQ"
      },
      "id": "HZkhPctO10SQ"
    },
    {
      "cell_type": "code",
      "source": [
        "class FirstHalf(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg3.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "    self.FNN = nn.Sequential(\n",
        "        nn.Linear(3000, 2048),\n",
        "        nn.LayerNorm(2048),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(2048, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "    )\n",
        "\n",
        "  def forward(self, ground_view, synthetic_aerial, segmented_aerial, candidate_aerial):\n",
        "    #x_ground = self.vgg1(ground_view)\n",
        "    x_segmented = self.vgg1(segmented_aerial)\n",
        "    x_synthetic = self.vgg2(synthetic_aerial)\n",
        "    x_candidate = self.vgg2(candidate_aerial)\n",
        "\n",
        "    x = torch.cat((x_synthetic, x_segmented, x_candidate), dim=-1)\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ZGMfYo512Bz9"
      },
      "id": "ZGMfYo512Bz9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Half"
      ],
      "metadata": {
        "id": "hDh1-Kx6HaDt"
      },
      "id": "hDh1-Kx6HaDt"
    },
    {
      "cell_type": "code",
      "source": [
        "class SecondHalf(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vgg1 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "    self.vgg2 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
        "\n",
        "    # Freezing initial layers for finetuning\n",
        "    for param1, param2 in zip(self.vgg1.features.parameters(), self.vgg2.features.parameters()):#, self.vgg1.features.parameters()):\n",
        "      param1.requires_grad = False\n",
        "      param2.requires_grad = False\n",
        "\n",
        "    # If needed can modify size of final output...\n",
        "    #self.vgg1.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg2.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "    #self.vgg3.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
        "\n",
        "    # Feed Forward Network turns output of VGG into embedding # TODO: decide final size...\n",
        "    self.FNN = nn.Sequential(\n",
        "        nn.Linear(2000, 1024),\n",
        "        nn.LayerNorm(1024),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.Linear(1024, 512)\n",
        "    )\n",
        "\n",
        "  def forward(self, ground_view, segmented_ground):\n",
        "    x_ground = self.vgg2(ground_view)\n",
        "    x_segmented = self.vgg3(segmented_ground)\n",
        "\n",
        "    x = torch.cat((x_ground, x_segmented), dim=-1)\n",
        "    x = self.FNN(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "sOJU_2qUHgSo"
      },
      "id": "sOJU_2qUHgSo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete Network"
      ],
      "metadata": {
        "id": "1b6MaaxAMktk"
      },
      "id": "1b6MaaxAMktk"
    },
    {
      "cell_type": "code",
      "source": [
        "class CompNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.FH = FirstHalf()\n",
        "    self.SH = SecondHalf()\n",
        "\n",
        "  def forward(self, ground_view, segmented_ground, synthetic_aerial, segmented_aerial, candidate_aerial):\n",
        "    return self.FH(ground_view, segmented_ground), self.SH(synthetic_aerial, segmented_aerial, candidate_aerial)"
      ],
      "metadata": {
        "id": "L-rqm6NRM1RJ"
      },
      "id": "L-rqm6NRM1RJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Triplet Loss"
      ],
      "metadata": {
        "id": "rTCQCntdCfw7"
      },
      "id": "rTCQCntdCfw7"
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedSoftMarginTripletLoss(nn.Module):\n",
        "  def __init__(self, margin=0.2):\n",
        "    super().__init__()\n",
        "    self.margin = margin\n",
        "\n",
        "  def forward(self, anchor, positive, negatives):\n",
        "\n",
        "    first = torch.norm(anchor - positive, dim=-1, keepdim=True)\n",
        "    second = -torch.norm(anchor - negatives, dim=-1, keepdim=True)\n",
        "    arg = self.margin * (second+first)\n",
        "    const = torch.zeros((arg.shape[0],1))\n",
        "    arg = torch.cat((const,arg), dim=-1)\n",
        "\n",
        "    return torch.logsumexp(arg, dim=-1).mean()\n"
      ],
      "metadata": {
        "id": "HwWxmTnMCkrZ"
      },
      "id": "HwWxmTnMCkrZ",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "KiKP8hBywFAV"
      },
      "id": "KiKP8hBywFAV"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion=WeightedSoftMarginTripletLoss(), scheduler=None):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, segmented_ground, synthetic_aerial, segmented_aerial in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      segmented_ground = segmented_ground.to(device)\n",
        "      synthetic_aerial = synthetic_aerial.to(device)\n",
        "      segmented_aerial =  segmented_aerial.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      optimizer.zero_grad()       # resets gradients from previous batch\n",
        "      labels, predictions = model( ground, segmented_ground, synthetic_aerial, segmented_aerial, aerial)\n",
        "      #print(aerial_pred.shape)\n",
        "\n",
        "      batch_size = len(labels)\n",
        "      batch_loss = 0.0\n",
        "\n",
        "      for i in range(batch_size):\n",
        "        indeces = range(batch_size)\n",
        "        indeces.pop(i)\n",
        "        anchor = labels[i]\n",
        "        positive = predictions[i]\n",
        "        negatives = predictions(indeces)\n",
        "\n",
        "        # Sanity check\n",
        "        if negatives.shape[0] != batch_size-1:\n",
        "          print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
        "\n",
        "        batch_loss += criterion(anchor, positive, negatives)\n",
        "        #Backward pass\n",
        "      batch_loss.backward()         # computes gradients via backpropagation\n",
        "      optimizer.step()        # updates weights using gradients\n",
        "\n",
        "      tot_loss += batch_loss.item()/batch_size\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device, criterion=WeightedSoftMarginTripletLoss()):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, segmented_ground, synthetic_aerial, segmented_aerial in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      segmented_ground = segmented_ground.to(device)\n",
        "      synthetic_aerial = synthetic_aerial.to(device)\n",
        "      segmented_aerial =  segmented_aerial.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      optimizer.zero_grad()       # resets gradients from previous batch\n",
        "      labels, predictions = model( ground, segmented_ground, synthetic_aerial, segmented_aerial, aerial)\n",
        "      #print(aerial_pred.shape)\n",
        "\n",
        "      batch_size = len(labels)\n",
        "      batch_loss = 0.0\n",
        "\n",
        "      for i in range(batch_size):\n",
        "        indeces = range(batch_size)\n",
        "        indeces.pop(i)\n",
        "        anchor = labels[i]\n",
        "        positive = predictions[i]\n",
        "        negatives = predictions(indeces)\n",
        "\n",
        "        # Sanity check\n",
        "        if negatives.shape[0] != batch_size-1:\n",
        "          print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
        "\n",
        "        batch_loss += criterion(anchor, positive, negatives)\n",
        "\n",
        "      tot_loss += batch_loss.item()/batch_size\n",
        "\n",
        "    return tot_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "hmwUIftSwEWI"
      },
      "id": "hmwUIftSwEWI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ],
      "metadata": {
        "id": "hwFdbzaf2xDH"
      },
      "id": "hwFdbzaf2xDH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "92440dfe"
      ]
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bac5fbf5a00b4529b113aa1e496d3cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b99738f91bed4fadbc0ac380acba6199",
              "IPY_MODEL_f42690cd3bc849f1891ea23bcd70ec8a",
              "IPY_MODEL_df50676124374335826a8a2abb1940bc"
            ],
            "layout": "IPY_MODEL_edfe68800a4b45499550e278322be115"
          }
        },
        "b99738f91bed4fadbc0ac380acba6199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78bc58f6143243f2a184b18b3ba551c6",
            "placeholder": "​",
            "style": "IPY_MODEL_4b3269f2a5af4292818fedb673ee1a7e",
            "value": "config.json: 100%"
          }
        },
        "f42690cd3bc849f1891ea23bcd70ec8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4bf1ac970a045f59f1b92a7434d4421",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_830129a60a8b454d8b64a543d5f6f4fa",
            "value": 502
          }
        },
        "df50676124374335826a8a2abb1940bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3914bf7bf5604da7b082d6412861a7b6",
            "placeholder": "​",
            "style": "IPY_MODEL_98cbfc471e12416caba8e7b5162c61ae",
            "value": " 502/502 [00:00&lt;00:00, 18.4kB/s]"
          }
        },
        "edfe68800a4b45499550e278322be115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78bc58f6143243f2a184b18b3ba551c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b3269f2a5af4292818fedb673ee1a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4bf1ac970a045f59f1b92a7434d4421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830129a60a8b454d8b64a543d5f6f4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3914bf7bf5604da7b082d6412861a7b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98cbfc471e12416caba8e7b5162c61ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4d4f316ff7c4154a42465eeb79fe024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e287b5cd4fc40bc8129f51b5a5c75d9",
              "IPY_MODEL_1a899b71ba4544cd94c733742fcd5f06",
              "IPY_MODEL_47a537182fdf4cd4ab53d616447d36e4"
            ],
            "layout": "IPY_MODEL_7e78b0e120044946bc2b341d4fa60bb3"
          }
        },
        "1e287b5cd4fc40bc8129f51b5a5c75d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a6bad5f838a4d9aad0f5dd3b10eaa1b",
            "placeholder": "​",
            "style": "IPY_MODEL_96b56669297e4418ac29b46ff1b5242a",
            "value": "model.safetensors: 100%"
          }
        },
        "1a899b71ba4544cd94c733742fcd5f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcda66cbe8ed446c9c8a19fa4c722496",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28386b597ba941cc8065513d7aa540c3",
            "value": 345579424
          }
        },
        "47a537182fdf4cd4ab53d616447d36e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33200016f1f14680b7a8d598633e4e1e",
            "placeholder": "​",
            "style": "IPY_MODEL_27965d019bd64a9e8812ec74faa02884",
            "value": " 346M/346M [00:09&lt;00:00, 64.0MB/s]"
          }
        },
        "7e78b0e120044946bc2b341d4fa60bb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a6bad5f838a4d9aad0f5dd3b10eaa1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b56669297e4418ac29b46ff1b5242a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcda66cbe8ed446c9c8a19fa4c722496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28386b597ba941cc8065513d7aa540c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33200016f1f14680b7a8d598633e4e1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27965d019bd64a9e8812ec74faa02884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}