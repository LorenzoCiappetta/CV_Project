{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4261e7ce",
      "metadata": {
        "id": "4261e7ce"
      },
      "source": [
        "# **Transformer model**\n",
        "From query groumd view images generate saellite images (natural and segmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b877e253",
      "metadata": {
        "id": "b877e253"
      },
      "source": [
        "**TODO**:\n",
        "- rimuovere segmentation maps delle ground view images del dataset (non servono)\n",
        "- modello pre-trained su satellite images per generare segmentation maps --> aggiungere al dataset le segmentation maps delle aerial images (ground truth per la generazione di segmentation maps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a26b9d6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26b9d6c",
        "outputId": "b1a9d189-6acb-477f-9844-8a55f3c7df34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch torchvision transformers scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1ea87d6",
      "metadata": {
        "id": "a1ea87d6"
      },
      "outputs": [],
      "source": [
        "def is_colab():\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "756e2bc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "756e2bc5",
        "outputId": "2b8b10dc-be2a-43f8-c76f-30f51d455ba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, ViTConfig\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "#import segmentation_models_pytorch as smp\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if is_colab():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "428b0422",
      "metadata": {
        "id": "428b0422"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a30d127",
      "metadata": {
        "id": "6a30d127"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "90e39bbf",
      "metadata": {
        "id": "90e39bbf"
      },
      "outputs": [],
      "source": [
        "class CVUSADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, ground_dir, aerial_dir, triplet_list, img_size=224, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ground_dir: Directory with all the ground view images\n",
        "            aerial_dir: Directory with all the ground aerial images\n",
        "            split: 'train', 'val' or 'test'\n",
        "            img_size: Size for images, 224x224\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "        \"\"\"\n",
        "\n",
        "        self.ground_dir = ground_dir\n",
        "        self.aerial_dir = aerial_dir\n",
        "        self.triplet_list = triplet_list\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Default transforms if none provided\n",
        "        if transform is None:\n",
        "            # For ground view images\n",
        "            self.ground_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # For aerial images (we might want different processing)\n",
        "            self.aerial_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])     # standard ImageNet normalization\n",
        "            ])\n",
        "            # Segmentation transform (nearest neighbor resize)\n",
        "            self.segmentation_transform = transforms.Compose([\n",
        "                transforms.Resize((img_size, img_size),\n",
        "                interpolation=transforms.InterpolationMode.NEAREST),\n",
        "                transforms.PILToTensor(),\n",
        "                transforms.Lambda(lambda x: x.squeeze(0).long())  # (H, W) int64 tensor\n",
        "            ])\n",
        "        else:\n",
        "            self.ground_transform = transform\n",
        "            self.aerial_transform = transform\n",
        "            self.segmentation_transform = transform\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        aerial_rel, ground_rel, seg_rel = self.triplet_list[idx]\n",
        "\n",
        "        # Load images\n",
        "        ground_img = Image.open(self.ground_dir + ground_rel)\n",
        "        aerial_img = Image.open(self.aerial_dir + aerial_rel)\n",
        "        seg_map = Image.open(self.ground_dir + seg_rel)\n",
        "\n",
        "        # Apply transforms\n",
        "        ground_tensor = self.ground_transform(ground_img)\n",
        "        aerial_tensor = self.aerial_transform(aerial_img)\n",
        "        seg_tensor = self.segmentation_transform(seg_map)  # Shape [H, W]\n",
        "\n",
        "        return ground_tensor, aerial_tensor, seg_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "97369ebc",
      "metadata": {
        "id": "97369ebc"
      },
      "outputs": [],
      "source": [
        "def read_triplets_csv(csv_path):\n",
        "    \"\"\"Reads CSV file into list of (aerial, ground, seg) triplets\"\"\"\n",
        "    triplets = []\n",
        "    with open(csv_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            triplets.append((\n",
        "                parts[0].strip(),  # aerial path\n",
        "                parts[1].strip(),  # ground path\n",
        "                parts[2].strip()   # seg path (ground view segmented map)\n",
        "            ))\n",
        "    return triplets\n",
        "\n",
        "\n",
        "if is_colab():\n",
        "    ground_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"/content/drive/MyDrive/CV_dataset/CVPR_subset/bingmap/\"\n",
        "else:\n",
        "    ground_dir = \"./CV_dataset/CVPR_subset/streetview/\"\n",
        "    aerial_dir = \"./CV_dataset/CVPR_subset/bingmap/\"\n",
        "\n",
        "\n",
        "train_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/train-19zl.csv\")\n",
        "train_triplets, val_triplets = train_test_split(train_triplets, test_size=0.15, random_state=19)  # training/validation set\n",
        "test_triplets = read_triplets_csv(\"/content/drive/MyDrive/CV_dataset/CVPR_subset/splits/splits/val-19zl.csv\")        # test set\n",
        "\n",
        "train_dataset = CVUSADataset(ground_dir, aerial_dir, train_triplets)\n",
        "val_dataset = CVUSADataset(ground_dir, aerial_dir, val_triplets)\n",
        "test_dataset = CVUSADataset(ground_dir, aerial_dir, test_triplets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df24aeab",
      "metadata": {
        "id": "df24aeab"
      },
      "source": [
        "## model  \n",
        "\n",
        "```\n",
        "                                                       ---> Aerial Decoder  \n",
        "                                                     /  \n",
        "Ground Image --> Patch Embedding --> ViT Encoder ---  \n",
        "                                                     \\  \n",
        "                                                       ---> Segmentation Decoder  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a6ee57cd",
      "metadata": {
        "id": "a6ee57cd"
      },
      "outputs": [],
      "source": [
        "class GroundToAerialTransformer(nn.Module):\n",
        "    def __init__(self, num_seg_classes=7, pretrained=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_seg_classes: Number of segmentation classes\n",
        "            pretrained: Use pretrained ViT weights\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # ViT Encoder (shared backbone)\n",
        "        model_name = 'google/vit-base-patch16-224-in21k'        # ViT base model, 16x16 patches, 224x224 input size\n",
        "        self.vit_config = ViTConfig.from_pretrained(model_name)\n",
        "        if pretrained:\n",
        "            self.vit = ViTModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.vit = ViTModel(self.vit_config)\n",
        "\n",
        "        # Aerial Image Decoder\n",
        "        self.aerial_decoder = nn.Sequential(\n",
        "            # First upsample to 14x14 (from 197x768)\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 512, kernel_size=2, stride=2),      # convolution\n",
        "            nn.BatchNorm2d(512),        # batch normalization\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 28x28\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Upsample to 56x56\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample to 224x224\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),     # 3 output channels (RGB)\n",
        "            nn.Tanh()  # Output in [-1, 1] range\n",
        "        )\n",
        "\n",
        "        # Segmentation Head\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            # First upsample\n",
        "            nn.ConvTranspose2d(self.vit_config.hidden_size, 256, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second upsample\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Third upsample\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Final upsample\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Conv2d(32, num_seg_classes, kernel_size=3, padding=1),       # num_seg_classes output channels (number of segmentation classes)\n",
        "            nn.Softmax(dim=1)  # Multi-class probabilities\n",
        "        )\n",
        "\n",
        "        # Learnable positional embedding for aerial reconstruction\n",
        "        self.aerial_pos_embed = nn.Parameter(torch.zeros(1, 196, self.vit_config.hidden_size))      # 196 = 14x14 (number of patches)\n",
        "        nn.init.trunc_normal_(self.aerial_pos_embed, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode ground image with ViT (process image into patch of tokens)\n",
        "        vit_outputs = self.vit(x)       # Output shape: [batch, 197, hidden_size]\n",
        "\n",
        "        last_hidden_state = vit_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        # remove CLS token for image generation (ViT outputs [CLS] token + 196 patch tokens)\n",
        "        aerial_tokens = last_hidden_state[:, 1:]\n",
        "\n",
        "        # add learned positional embedding for aerial structure\n",
        "        aerial_tokens = aerial_tokens + self.aerial_pos_embed\n",
        "\n",
        "        # Reshape to spatial dimensions (14x14)\n",
        "        batch_size = aerial_tokens.size(0)\n",
        "        aerial_tokens = aerial_tokens.view(batch_size, 14, 14, -1)      # convert 1D sequence into 2D spatial grid. shape becomes: (batch_size, 14, 14, hidden_size)\n",
        "        aerial_tokens = aerial_tokens.permute(0, 3, 1, 2)  # permute shape: (batch_size, hidden_size, 14, 14)\n",
        "\n",
        "        print(aerial_tokens.shape)\n",
        "        # Decode aerial image\n",
        "        aerial_output = self.aerial_decoder(aerial_tokens)\n",
        "        print(aerial_output.shape)\n",
        "\n",
        "        # Decode segmentation map\n",
        "        #seg_output = self.segmentation_head(aerial_tokens)\n",
        "\n",
        "\n",
        "        return aerial_output#, seg_output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92440dfe",
      "metadata": {
        "id": "92440dfe"
      },
      "source": [
        "\n",
        "## training w/ segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b21423",
      "metadata": {
        "id": "b2b21423"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).cuda()\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true, seg_pred, seg_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    seg_loss = seg_loss_fn(seg_pred, seg_true)\n",
        "    # Weighted combination\n",
        "    return 0.7 * img_loss + 0.3 * seg_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98987bb4",
      "metadata": {
        "id": "98987bb4"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ground, (aerial, seg) in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred, seg_pred = model(ground)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial, seg_pred, seg)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3QwZksLzdhul",
      "metadata": {
        "id": "3QwZksLzdhul"
      },
      "source": [
        "## Training no segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "OQXLQ8oLdoi4",
      "metadata": {
        "id": "OQXLQ8oLdoi4"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = GroundToAerialTransformer(num_seg_classes=5).to(device)\n",
        "\n",
        "# Loss functions\n",
        "aerial_loss_fn = nn.L1Loss()  # For aerial images\n",
        "seg_loss_fn = nn.CrossEntropyLoss()  # For segmentation\n",
        "\n",
        "# Combined loss with weighting\n",
        "def total_loss(aerial_pred, aerial_true):\n",
        "    # Image reconstruction loss\n",
        "    img_loss = aerial_loss_fn(aerial_pred, aerial_true)\n",
        "    # Segmentation loss\n",
        "    # Weighted combination\n",
        "    return img_loss\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.vit.parameters(), 'lr': 5e-5},  # Lower LR for pretrained (fine-tuning)\n",
        "    {'params': model.aerial_decoder.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.segmentation_head.parameters(), 'lr': 1e-4},\n",
        "    {'params': model.aerial_pos_embed, 'lr': 1e-4}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Learning rate scheduler (adjust learning rate during training)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "4vEfA5TrdrEi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "4vEfA5TrdrEi",
        "outputId": "4ca60477-9cf3-4e45-e837-45aec4e8b84a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 768, 14, 14])\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 768, 14, 14])\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 768, 14, 14])\n",
            "torch.Size([8, 3, 224, 224])\n",
            "torch.Size([8, 3, 224, 224])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-42-1560205210.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-42-1560205210.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maerial_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maerial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# computes gradients via backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# updates weights using gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "\n",
        "    for ground, aerial, _ in dataloader:\n",
        "        ground = ground.to(device)\n",
        "        aerial = aerial.to(device)\n",
        "        #seg = seg.to(device)  # Assuming seg is preprocessed\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()       # resets gradients from previous batch\n",
        "        aerial_pred = model(ground)\n",
        "        print(aerial_pred.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = total_loss(aerial_pred, aerial)\n",
        "        # Backward pass\n",
        "        loss.backward()         # computes gradients via backpropagation\n",
        "        optimizer.step()        # updates weights using gradients\n",
        "\n",
        "        tot_loss += loss.item()\n",
        "\n",
        "    return tot_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "  model.eval()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for ground, aerial, _ in dataloader:\n",
        "      ground = ground.to(device)\n",
        "      aerial = aerial.to(device)\n",
        "      aerial_pred = model(ground)\n",
        "\n",
        "      loss = total_loss(aerial_pred, aerial)\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "\n",
        "# Main training\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "    val_loss = evaluate(model, val_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()    # adjusts learning rate after each epoch\n",
        "\n",
        "    # Save checkpoint\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9IpzLyOog-jX",
      "metadata": {
        "id": "9IpzLyOog-jX"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cl1vAKgqhCny",
      "metadata": {
        "id": "Cl1vAKgqhCny"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for ground, (aerial, _) in test_loader:\n",
        "    ground = ground.to(device)\n",
        "    aerial = aerial.to(device)\n",
        "    aerial_pred = model(ground)\n",
        "\n",
        "    aerial_pred = torchvision.transforms.functional.to_pil_image(aerial_pred, mode=None)\n",
        "    aerial = torchvision.transforms.functional.to_pil_image(aerial, mode=None)\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title('Satellite Image RGB')\n",
        "    plt.imshow(aerial)     # Original full-size image\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(aerial_pred, cmap='viridis')\n",
        "    plt.title('Satellite prediction')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "92440dfe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
